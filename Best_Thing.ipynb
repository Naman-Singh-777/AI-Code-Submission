{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IH68pPK-w6JN",
        "outputId": "35ec453c-6f3d-4fc4-ae14-4746bbcf4b48"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install pandas numpy PyPDF2 scikit-learn spacy nltk tensorflow\n",
        "\n",
        "# Import all necessary libraries\n",
        "import os\n",
        "import json\n",
        "import uuid\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import PyPDF2  # Correct import (not PyPDF2)\n",
        "from datetime import datetime\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "import spacy\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import joblib\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model, save_model\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "!pip install matplotlib seaborn networkx\n",
        "# Download required NLTK data\n",
        "# First, let's download the regular punkt data\n",
        "import nltk\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Download the standard punkt tokenizer\n",
        "\n",
        "\n",
        "# Create the necessary directory structure for punkt_tab\n",
        "nltk_data_path = nltk.data.path[0]  # Get the first NLTK data path\n",
        "punkt_path = os.path.join(nltk_data_path, 'tokenizers', 'punkt', 'english')\n",
        "punkt_tab_dir = os.path.join(nltk_data_path, 'tokenizers', 'punkt_tab', 'english')\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(punkt_tab_dir, exist_ok=True)\n",
        "\n",
        "# The specific files needed are collocations.tab and sent_starters.txt\n",
        "files_to_copy = ['collocations.tab', 'sent_starters.txt']\n",
        "\n",
        "for file in files_to_copy:\n",
        "    source_file = os.path.join(punkt_path, file)\n",
        "    target_file = os.path.join(punkt_tab_dir, file)\n",
        "\n",
        "    # Check if source files exist\n",
        "    if os.path.exists(source_file):\n",
        "        # Copy the file\n",
        "        shutil.copy2(source_file, target_file)\n",
        "        print(f\"Copied {file} to {punkt_tab_dir}\")\n",
        "    else:\n",
        "        # Create empty files if originals don't exist\n",
        "        with open(target_file, 'w', encoding='utf-8') as f:\n",
        "            print(f\"Created empty {file} in {punkt_tab_dir}\")\n",
        "\n",
        "print(\"Setup complete for punkt_tab resources\")\n",
        "# Initialize spaCy\n",
        "try:\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "except:\n",
        "    # If model not available, download it\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Initialize Porter Stemmer\n",
        "stemmer = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-Qg_q8Ow7gS"
      },
      "outputs": [],
      "source": [
        "def generate_career_database(output_path='career_database.csv'):\n",
        "    \"\"\"\n",
        "    Generates a CSV file with career data for engineering graduates\n",
        "    \"\"\"\n",
        "    # Common career paths after engineering degrees\n",
        "    job_titles = [\n",
        "        'Software Engineer', 'Senior Software Engineer', 'Frontend Developer', 'Backend Developer',\n",
        "        'Full-Stack Developer', 'Mobile App Developer', 'Data Scientist', 'Data Analyst',\n",
        "        'Machine Learning Engineer', 'AI Researcher', 'DevOps Engineer', 'Cloud Architect',\n",
        "        'Systems Administrator', 'Network Engineer', 'Security Engineer', 'QA Engineer',\n",
        "        'Test Automation Engineer', 'Product Manager', 'Project Manager', 'Scrum Master',\n",
        "        'Business Analyst', 'Technical Consultant', 'Solutions Architect', 'Database Administrator',\n",
        "        'Technical Writer', 'UX/UI Designer', 'Game Developer', 'Blockchain Developer',\n",
        "        'IoT Developer', 'AR/VR Developer'\n",
        "    ]\n",
        "\n",
        "    # Common industries\n",
        "    industries = [\n",
        "        'Technology', 'Healthcare', 'Finance', 'Education', 'E-commerce',\n",
        "        'Manufacturing', 'Telecommunications', 'Entertainment', 'Consulting',\n",
        "        'Energy', 'Transportation', 'Government', 'Defense', 'Retail'\n",
        "    ]\n",
        "\n",
        "    # Career stages\n",
        "    career_stages = ['entry-level', 'early-career', 'mid-career', 'experienced', 'senior']\n",
        "\n",
        "    # Technical skills pool\n",
        "    technical_skills = [\n",
        "        'Python', 'Java', 'JavaScript', 'C++', 'C#', 'Go', 'Rust', 'Swift', 'Kotlin',\n",
        "        'TypeScript', 'PHP', 'Ruby', 'SQL', 'HTML', 'CSS', 'React', 'Angular', 'Vue',\n",
        "        'Node.js', 'Django', 'Flask', 'Spring', 'ASP.NET', 'Express.js', 'Ruby on Rails',\n",
        "        'TensorFlow', 'PyTorch', 'scikit-learn', 'Pandas', 'NumPy', 'Docker', 'Kubernetes',\n",
        "        'AWS', 'Azure', 'GCP', 'Git', 'CI/CD', 'Jenkins', 'Terraform', 'Ansible',\n",
        "        'Hadoop', 'Spark', 'MongoDB', 'PostgreSQL', 'MySQL', 'Redis', 'Elasticsearch',\n",
        "        'GraphQL', 'REST API', 'Microservices', 'Serverless', 'Linux', 'Blockchain',\n",
        "        'IoT', 'AR/VR', 'Unity', 'Unreal Engine', 'WebGL', 'Mobile Development'\n",
        "    ]\n",
        "\n",
        "    # Soft skills pool\n",
        "    soft_skills = [\n",
        "        'Communication', 'Leadership', 'Teamwork', 'Problem Solving', 'Critical Thinking',\n",
        "        'Time Management', 'Adaptability', 'Creativity', 'Attention to Detail', 'Negotiation',\n",
        "        'Conflict Resolution', 'Emotional Intelligence', 'Presentation', 'Client Management',\n",
        "        'Mentoring', 'Decision Making', 'Organization', 'Delegation', 'Strategic Thinking',\n",
        "        'Research', 'Analytical Skills'\n",
        "    ]\n",
        "\n",
        "    # Create different profiles for new job seekers vs. career transitioners\n",
        "    # New job seekers typically need fewer specialized skills\n",
        "    new_seeker_profiles = {\n",
        "        'entry-level': {\n",
        "            'tech_skills': (3, 6),\n",
        "            'soft_skills': (2, 4),\n",
        "            'salary_range': (50000, 70000, 10000, 20000)\n",
        "        },\n",
        "        'early-career': {\n",
        "            'tech_skills': (4, 7),\n",
        "            'soft_skills': (3, 5),\n",
        "            'salary_range': (65000, 85000, 10000, 20000)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Career transitioners often bring more diverse skills but might lack some core technical skills\n",
        "    transitioner_profiles = {\n",
        "        'entry-level': {\n",
        "            'tech_skills': (2, 5),\n",
        "            'soft_skills': (4, 7),\n",
        "            'salary_range': (55000, 75000, 10000, 20000)\n",
        "        },\n",
        "        'early-career': {\n",
        "            'tech_skills': (4, 8),\n",
        "            'soft_skills': (5, 8),\n",
        "            'salary_range': (70000, 90000, 15000, 25000)\n",
        "        },\n",
        "        'mid-career': {\n",
        "            'tech_skills': (5, 9),\n",
        "            'soft_skills': (6, 9),\n",
        "            'salary_range': (85000, 115000, 20000, 30000)\n",
        "        },\n",
        "        'experienced': {\n",
        "            'tech_skills': (6, 10),\n",
        "            'soft_skills': (7, 10),\n",
        "            'salary_range': (110000, 140000, 25000, 40000)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Create the careers data\n",
        "    careers = []\n",
        "    career_id = 1\n",
        "\n",
        "    # Generate for new job seekers\n",
        "    for i in range(40):  # 40 career entries for new job seekers\n",
        "        job_title = random.choice(job_titles)\n",
        "        industry = random.choice(industries)\n",
        "        career_stage = random.choice(list(new_seeker_profiles.keys()))\n",
        "        career_type = 'new_seeker'\n",
        "\n",
        "        profile = new_seeker_profiles[career_stage]\n",
        "\n",
        "        # Select skills based on profile\n",
        "        num_tech_skills = random.randint(*profile['tech_skills'])\n",
        "        num_soft_skills = random.randint(*profile['soft_skills'])\n",
        "\n",
        "        required_tech_skills = random.sample(technical_skills, num_tech_skills)\n",
        "        required_soft_skills = random.sample(soft_skills, num_soft_skills)\n",
        "\n",
        "        # Add job-specific skills\n",
        "        if 'Software' in job_title or 'Developer' in job_title:\n",
        "            job_specific_skills = ['Programming', 'Software Development', 'Debugging']\n",
        "            required_tech_skills.extend(random.sample(job_specific_skills, min(2, len(job_specific_skills))))\n",
        "        elif 'Data' in job_title:\n",
        "            job_specific_skills = ['Statistics', 'Data Visualization', 'Data Analysis']\n",
        "            required_tech_skills.extend(random.sample(job_specific_skills, min(2, len(job_specific_skills))))\n",
        "\n",
        "        # All required skills\n",
        "        all_required_skills = required_tech_skills + required_soft_skills\n",
        "\n",
        "        # Salary range\n",
        "        min_salary = random.randint(*profile['salary_range'][:2])\n",
        "        max_salary = min_salary + random.randint(*profile['salary_range'][2:])\n",
        "\n",
        "        # Create job description\n",
        "        job_description = f\"Entry-level position suitable for graduates with minimal experience. Requires proficiency in {', '.join(required_tech_skills[:3])}. Strong {', '.join(required_soft_skills[:2])} skills are essential. This role provides excellent opportunities for growth and skill development.\"\n",
        "\n",
        "        career = {\n",
        "            'job_id': career_id,\n",
        "            'job_title': job_title,\n",
        "            'industry': industry,\n",
        "            'career_stage': career_stage,\n",
        "            'career_type': career_type,\n",
        "            'required_skills': '|'.join(all_required_skills),\n",
        "            'min_salary': min_salary,\n",
        "            'max_salary': max_salary,\n",
        "            'job_description': job_description,\n",
        "            'company_size': random.choice(['Small', 'Medium', 'Large', 'Enterprise']),\n",
        "            'remote_options': random.choice(['Remote', 'Hybrid', 'On-site']),\n",
        "            'growth_potential': random.randint(7, 10)  # New job seekers have high growth potential\n",
        "        }\n",
        "\n",
        "        careers.append(career)\n",
        "        career_id += 1\n",
        "\n",
        "    # Generate for career transitioners\n",
        "    for i in range(60):  # 60 career entries for career transitioners\n",
        "        job_title = random.choice(job_titles)\n",
        "        industry = random.choice(industries)\n",
        "        career_stage = random.choice(list(transitioner_profiles.keys()))\n",
        "        career_type = 'transitioner'\n",
        "\n",
        "        profile = transitioner_profiles[career_stage]\n",
        "\n",
        "        # Select skills based on profile\n",
        "        num_tech_skills = random.randint(*profile['tech_skills'])\n",
        "        num_soft_skills = random.randint(*profile['soft_skills'])\n",
        "\n",
        "        required_tech_skills = random.sample(technical_skills, num_tech_skills)\n",
        "        required_soft_skills = random.sample(soft_skills, num_soft_skills)\n",
        "\n",
        "        # Add job-specific skills\n",
        "        if 'Manager' in job_title or 'Consultant' in job_title:\n",
        "            job_specific_skills = ['Project Management', 'Stakeholder Management', 'Strategic Planning']\n",
        "            required_tech_skills.extend(random.sample(job_specific_skills, min(2, len(job_specific_skills))))\n",
        "        elif 'Architect' in job_title:\n",
        "            job_specific_skills = ['System Design', 'Architecture Patterns', 'Technical Leadership']\n",
        "            required_tech_skills.extend(random.sample(job_specific_skills, min(2, len(job_specific_skills))))\n",
        "\n",
        "        # All required skills\n",
        "        all_required_skills = required_tech_skills + required_soft_skills\n",
        "\n",
        "        # Salary range\n",
        "        min_salary = random.randint(*profile['salary_range'][:2])\n",
        "        max_salary = min_salary + random.randint(*profile['salary_range'][2:])\n",
        "\n",
        "        # Create job description\n",
        "        job_description = f\"Position ideal for professionals transitioning from related fields. Values transferrable skills like {', '.join(required_soft_skills[:3])}. Technical requirements include {', '.join(required_tech_skills[:3])}. Previous experience in similar domains can substitute for some technical requirements.\"\n",
        "\n",
        "        career = {\n",
        "            'job_id': career_id,\n",
        "            'job_title': job_title,\n",
        "            'industry': industry,\n",
        "            'career_stage': career_stage,\n",
        "            'career_type': career_type,\n",
        "            'required_skills': '|'.join(all_required_skills),\n",
        "            'min_salary': min_salary,\n",
        "            'max_salary': max_salary,\n",
        "            'job_description': job_description,\n",
        "            'company_size': random.choice(['Small', 'Medium', 'Large', 'Enterprise']),\n",
        "            'remote_options': random.choice(['Remote', 'Hybrid', 'On-site']),\n",
        "            'growth_potential': random.randint(5, 9)  # Slightly lower average growth potential for transitioners\n",
        "        }\n",
        "\n",
        "        careers.append(career)\n",
        "        career_id += 1\n",
        "\n",
        "    # Create DataFrame and save to CSV\n",
        "    careers_df = pd.DataFrame(careers)\n",
        "    careers_df.to_csv(output_path, index=False)\n",
        "    print(f\"Generated career database with {len(careers)} entries\")\n",
        "    return careers_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcFq_qjTw7iw"
      },
      "outputs": [],
      "source": [
        "def validate_pdf(file_path):\n",
        "    \"\"\"\n",
        "    Validates if the file is a valid PDF\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if file exists\n",
        "        if not os.path.exists(file_path):\n",
        "            return {\"valid\": False, \"error\": \"File does not exist\"}\n",
        "\n",
        "        # Check if file is a PDF\n",
        "        if not file_path.lower().endswith('.pdf'):\n",
        "            return {\"valid\": False, \"error\": \"File is not a PDF\"}\n",
        "\n",
        "        # Check file size\n",
        "        max_size = 10 * 1024 * 1024  # 10MB\n",
        "        if os.path.getsize(file_path) > max_size:\n",
        "            return {\"valid\": False, \"error\": \"File too large (max 10MB)\"}\n",
        "\n",
        "        # Try to open and read PDF\n",
        "        with open(file_path, 'rb') as f:\n",
        "            pdf_reader = PyPDF2.PdfReader(f)\n",
        "            if len(pdf_reader.pages) < 1:\n",
        "                return {\"valid\": False, \"error\": \"PDF has no pages\"}\n",
        "\n",
        "        return {\"valid\": True}\n",
        "    except Exception as e:\n",
        "        return {\"valid\": False, \"error\": str(e)}\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    \"\"\"\n",
        "    Extracts text from PDF file with positional data\n",
        "    \"\"\"\n",
        "    text_data = {\n",
        "        \"full_text\": \"\",\n",
        "        \"text_by_page\": [],\n",
        "        \"sections\": []\n",
        "    }\n",
        "\n",
        "    with open(file_path, 'rb') as f:\n",
        "        pdf_reader = PyPDF2.PdfReader(f)\n",
        "        num_pages = len(pdf_reader.pages)\n",
        "\n",
        "        for i in range(num_pages):\n",
        "            page = pdf_reader.pages[i]\n",
        "            page_text = page.extract_text()\n",
        "\n",
        "            text_data[\"full_text\"] += page_text + \" \"\n",
        "            text_data[\"text_by_page\"].append(page_text)\n",
        "\n",
        "            # Simple section detection based on line breaks and formatting\n",
        "            potential_sections = page_text.split('\\n\\n')\n",
        "\n",
        "            for section_text in potential_sections:\n",
        "                if section_text.strip():\n",
        "                    # Try to identify if this is a section header\n",
        "                    lines = section_text.split('\\n')\n",
        "                    if len(lines) > 0 and any(keyword in lines[0].lower() for keyword in\n",
        "                                             ['experience', 'education', 'skills', 'certification',\n",
        "                                              'projects', 'summary', 'objective']):\n",
        "                        section = {\n",
        "                            \"title\": lines[0].strip(),\n",
        "                            \"content\": '\\n'.join(lines[1:]) if len(lines) > 1 else \"\",\n",
        "                            \"page\": i\n",
        "                        }\n",
        "                        text_data[\"sections\"].append(section)\n",
        "\n",
        "    return text_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwlJACavw7lb"
      },
      "outputs": [],
      "source": [
        "def recognize_document_structure(text_data):\n",
        "    \"\"\"\n",
        "    Analyzes the resume structure to identify different sections\n",
        "    \"\"\"\n",
        "    # Define common section keywords\n",
        "    section_patterns = {\n",
        "        'contactInfo': ['contact', 'phone', 'email', 'address', 'linkedin'],\n",
        "        'summary': ['summary', 'objective', 'profile', 'about'],\n",
        "        'experience': ['experience', 'work', 'employment', 'job history', 'professional'],\n",
        "        'education': ['education', 'academic', 'qualification', 'degree', 'university', 'college'],\n",
        "        'skills': ['skills', 'expertise', 'technical', 'competencies', 'proficiencies'],\n",
        "        'projects': ['projects', 'portfolio', 'works'],\n",
        "        'certifications': ['certifications', 'certificates', 'licenses', 'credentials']\n",
        "    }\n",
        "\n",
        "    structured_sections = []\n",
        "\n",
        "    # Process sections from text extraction\n",
        "    for section in text_data[\"sections\"]:\n",
        "        section_type = \"other\"\n",
        "\n",
        "        # Check against patterns to determine section type\n",
        "        for type_name, keywords in section_patterns.items():\n",
        "            if any(keyword in section[\"title\"].lower() for keyword in keywords):\n",
        "                section_type = type_name\n",
        "                break\n",
        "\n",
        "        structured_sections.append({\n",
        "            \"title\": section[\"title\"],\n",
        "            \"content\": section[\"content\"],\n",
        "            \"type\": section_type,\n",
        "            \"page\": section[\"page\"]\n",
        "        })\n",
        "\n",
        "    # If no sections were found, try to extract them from full text\n",
        "    if not structured_sections:\n",
        "        full_text = text_data[\"full_text\"]\n",
        "\n",
        "        # Simple regex-based section extraction\n",
        "        for type_name, keywords in section_patterns.items():\n",
        "            pattern = '|'.join(keywords)\n",
        "            regex = rf'(?i)({pattern})(?:\\s|\\:|\\n)'\n",
        "\n",
        "            matches = re.finditer(regex, full_text)\n",
        "            for match in matches:\n",
        "                start_pos = match.start()\n",
        "                # Find the next section or end of text\n",
        "                next_match = re.search(regex, full_text[start_pos + 1:])\n",
        "                end_pos = start_pos + 1 + next_match.start() if next_match else len(full_text)\n",
        "\n",
        "                section_title = full_text[start_pos:start_pos + match.end() - start_pos].strip()\n",
        "                section_content = full_text[start_pos + len(section_title):end_pos].strip()\n",
        "\n",
        "                structured_sections.append({\n",
        "                    \"title\": section_title,\n",
        "                    \"content\": section_content,\n",
        "                    \"type\": type_name,\n",
        "                    \"page\": 0  # We don't know the page number in this case\n",
        "                })\n",
        "\n",
        "    # Extract contact information\n",
        "    contact_info = extract_contact_info(text_data[\"full_text\"])\n",
        "\n",
        "    return {\n",
        "        \"structured_sections\": structured_sections,\n",
        "        \"contact_info\": contact_info\n",
        "    }\n",
        "\n",
        "def extract_contact_info(text):\n",
        "    \"\"\"\n",
        "    Extracts contact information from resume text\n",
        "    \"\"\"\n",
        "    contact_info = {\n",
        "        \"email\": None,\n",
        "        \"phone\": None,\n",
        "        \"linkedin\": None,\n",
        "        \"website\": None,\n",
        "        \"location\": None\n",
        "    }\n",
        "\n",
        "    # Email regex\n",
        "    email_regex = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "    email_match = re.search(email_regex, text)\n",
        "    if email_match:\n",
        "        contact_info[\"email\"] = email_match.group(0)\n",
        "\n",
        "    # Phone regex\n",
        "    phone_regex = r'(?:\\+\\d{1,3}[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}'\n",
        "    phone_match = re.search(phone_regex, text)\n",
        "    if phone_match:\n",
        "        contact_info[\"phone\"] = phone_match.group(0)\n",
        "\n",
        "    # LinkedIn regex\n",
        "    linkedin_regex = r'(?:linkedin\\.com\\/in\\/)[a-zA-Z0-9_-]+'\n",
        "    linkedin_match = re.search(linkedin_regex, text)\n",
        "    if linkedin_match:\n",
        "        contact_info[\"linkedin\"] = linkedin_match.group(0)\n",
        "\n",
        "    # Website regex\n",
        "    website_regex = r'(?:https?:\\/\\/)?(?:www\\.)?[a-zA-Z0-9-]+\\.[a-zA-Z]{2,}(?:\\/[^\\s]*)?'\n",
        "    website_matches = re.findall(website_regex, text)\n",
        "    if website_matches:\n",
        "        # Filter out LinkedIn URLs\n",
        "        websites = [url for url in website_matches if 'linkedin' not in url]\n",
        "        if websites:\n",
        "            contact_info[\"website\"] = websites[0]\n",
        "\n",
        "    # Location - city, state format common in resumes\n",
        "    location_regex = r'\\b[A-Z][a-z]+(?:[\\s-][A-Z][a-z]+)*,\\s*[A-Z]{2}\\b'\n",
        "    location_match = re.search(location_regex, text)\n",
        "    if location_match:\n",
        "        contact_info[\"location\"] = location_match.group(0)\n",
        "\n",
        "    return contact_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAgaLuFDw7oy"
      },
      "outputs": [],
      "source": [
        "def extract_skills(structured_data):\n",
        "    \"\"\"\n",
        "    Extracts explicit and implicit skills from resume\n",
        "    \"\"\"\n",
        "    # Load the skill taxonomy (in a real implementation, this would be a comprehensive database)\n",
        "    skill_taxonomy = load_skill_taxonomy()\n",
        "\n",
        "    extracted_skills = {\n",
        "        \"technical\": [],\n",
        "        \"soft\": [],\n",
        "        \"languages\": [],\n",
        "        \"tools\": [],\n",
        "        \"methodologies\": [],\n",
        "        \"domain\": [],\n",
        "        \"inferred\": []\n",
        "    }\n",
        "\n",
        "    # Find skills section\n",
        "    skills_section = next((s for s in structured_data[\"structured_sections\"]\n",
        "                          if s[\"type\"] == \"skills\"), None)\n",
        "\n",
        "    # Find experience section for implicit skills\n",
        "    experience_section = next((s for s in structured_data[\"structured_sections\"]\n",
        "                              if s[\"type\"] == \"experience\"), None)\n",
        "\n",
        "    # Process explicit skills if skills section exists\n",
        "    if skills_section:\n",
        "        skills_text = skills_section[\"content\"]\n",
        "        explicit_skills = extract_explicit_skills(skills_text, skill_taxonomy)\n",
        "\n",
        "        # Categorize extracted skills\n",
        "        for skill_name in explicit_skills:\n",
        "            category = categorize_skill(skill_name, skill_taxonomy)\n",
        "            if category in extracted_skills:\n",
        "                extracted_skills[category].append({\n",
        "                    \"name\": skill_name,\n",
        "                    \"source\": \"explicit\",\n",
        "                    \"confidence\": 0.95\n",
        "                })\n",
        "\n",
        "    # Process experience section for implicit skills\n",
        "    if experience_section:\n",
        "        experience_text = experience_section[\"content\"]\n",
        "        implicit_skills = extract_implicit_skills(experience_text, skill_taxonomy)\n",
        "\n",
        "        # Add implicit skills with confidence scores\n",
        "        for skill_data in implicit_skills:\n",
        "            skill_name = skill_data[\"skill\"]\n",
        "            category = skill_data[\"category\"]\n",
        "            confidence = skill_data[\"confidence\"]\n",
        "\n",
        "            # Check if skill already exists in extracted skills\n",
        "            existing = False\n",
        "            for cat, skills in extracted_skills.items():\n",
        "                if any(s[\"name\"].lower() == skill_name.lower() for s in skills):\n",
        "                    existing = True\n",
        "                    break\n",
        "\n",
        "            if not existing and category in extracted_skills:\n",
        "                extracted_skills[category].append({\n",
        "                    \"name\": skill_name,\n",
        "                    \"source\": \"implicit\",\n",
        "                    \"confidence\": confidence\n",
        "                })\n",
        "\n",
        "        # Infer additional skills from experience\n",
        "        inferred_skills = infer_skills_from_experience(experience_text, skill_taxonomy)\n",
        "        extracted_skills[\"inferred\"] = inferred_skills\n",
        "\n",
        "    return extracted_skills\n",
        "\n",
        "def load_skill_taxonomy():\n",
        "    \"\"\"\n",
        "    Loads skill taxonomy (simplified version)\n",
        "    \"\"\"\n",
        "    taxonomy = {\n",
        "        \"technical\": [\n",
        "            \"Python\", \"Java\", \"JavaScript\", \"C++\", \"C#\", \"SQL\", \"HTML\", \"CSS\",\n",
        "            \"React\", \"Angular\", \"Vue\", \"Node.js\", \"Django\", \"Flask\", \"Spring\",\n",
        "            \"Machine Learning\", \"Deep Learning\", \"NLP\", \"Computer Vision\",\n",
        "            \"Data Analysis\", \"Statistics\", \"Algorithms\", \"Data Structures\"\n",
        "        ],\n",
        "        \"soft\": [\n",
        "            \"Communication\", \"Leadership\", \"Teamwork\", \"Problem Solving\",\n",
        "            \"Critical Thinking\", \"Time Management\", \"Adaptability\", \"Creativity\",\n",
        "            \"Attention to Detail\", \"Presentation\", \"Negotiation\", \"Mentoring\"\n",
        "        ],\n",
        "        \"languages\": [\n",
        "            \"English\", \"Spanish\", \"French\", \"German\", \"Chinese\", \"Japanese\",\n",
        "            \"Russian\", \"Portuguese\", \"Arabic\", \"Hindi\"\n",
        "        ],\n",
        "        \"tools\": [\n",
        "            \"Git\", \"Docker\", \"Kubernetes\", \"AWS\", \"Azure\", \"GCP\", \"JIRA\",\n",
        "            \"Confluence\", \"Slack\", \"MS Office\", \"Photoshop\", \"Figma\", \"Tableau\",\n",
        "            \"Power BI\", \"Excel\", \"Jenkins\", \"Travis CI\", \"CircleCI\"\n",
        "        ],\n",
        "        \"methodologies\": [\n",
        "            \"Agile\", \"Scrum\", \"Kanban\", \"Waterfall\", \"DevOps\", \"TDD\", \"BDD\",\n",
        "            \"CI/CD\", \"Lean\", \"Six Sigma\", \"Design Thinking\", \"OOP\", \"Functional Programming\"\n",
        "        ],\n",
        "        \"domain\": [\n",
        "            \"Finance\", \"Healthcare\", \"Education\", \"E-commerce\", \"Gaming\",\n",
        "            \"Social Media\", \"Cybersecurity\", \"Blockchain\", \"IoT\", \"Telecommunications\",\n",
        "            \"Logistics\", \"Manufacturing\", \"Retail\", \"Energy\", \"Transportation\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Create flat list for lookup\n",
        "    all_skills = []\n",
        "    for category, skills in taxonomy.items():\n",
        "        all_skills.extend([(skill.lower(), category) for skill in skills])\n",
        "\n",
        "    return {\n",
        "        \"categories\": taxonomy,\n",
        "        \"all_skills\": dict(all_skills)\n",
        "    }\n",
        "\n",
        "def extract_explicit_skills(text, skill_taxonomy):\n",
        "    \"\"\"\n",
        "    Extracts explicit skills from skills section\n",
        "    \"\"\"\n",
        "    skills = []\n",
        "\n",
        "    # Convert text to lowercase for case-insensitive matching\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    # Look for skills in taxonomy\n",
        "    for skill_name in skill_taxonomy[\"all_skills\"].keys():\n",
        "        # Check for exact match with word boundaries\n",
        "        pattern = r'\\b' + re.escape(skill_name) + r'\\b'\n",
        "        if re.search(pattern, text_lower):\n",
        "            # Capitalize skill name properly\n",
        "            words = skill_name.split()\n",
        "            capitalized = ' '.join(word.capitalize() for word in words)\n",
        "            skills.append(capitalized)\n",
        "\n",
        "    # Look for skills separated by commas, bullets, or newlines\n",
        "    items = re.split(r',|\\n|•|\\*|\\/|\\\\', text)\n",
        "    for item in items:\n",
        "        item = item.strip().lower()\n",
        "        if item in skill_taxonomy[\"all_skills\"]:\n",
        "            words = item.split()\n",
        "            capitalized = ' '.join(word.capitalize() for word in words)\n",
        "            skills.append(capitalized)\n",
        "\n",
        "    # Remove duplicates\n",
        "    return list(set(skills))\n",
        "\n",
        "def extract_implicit_skills(text, skill_taxonomy):\n",
        "    \"\"\"\n",
        "    Extracts implicit skills from experience descriptions\n",
        "    \"\"\"\n",
        "    implicit_skills = []\n",
        "\n",
        "    # Process text with spaCy for better phrase extraction\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Extract sentences\n",
        "    sentences = [sent.text for sent in doc.sents]\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Look for verbs followed by technical terms\n",
        "        sentence_doc = nlp(sentence)\n",
        "\n",
        "        for token in sentence_doc:\n",
        "            if token.pos_ == \"VERB\":\n",
        "                # Check next few tokens for potential skill\n",
        "                for i in range(1, 5):  # Look ahead up to 4 tokens\n",
        "                    if token.i + i >= len(sentence_doc):\n",
        "                        break\n",
        "\n",
        "                    # Get span of tokens that could be a skill phrase\n",
        "                    potential_skill = sentence_doc[token.i + 1:token.i + i + 1].text.lower()\n",
        "\n",
        "                    # Check against skill taxonomy\n",
        "                    for skill_name, category in skill_taxonomy[\"all_skills\"].items():\n",
        "                        similarity = calculate_skill_similarity(potential_skill, skill_name)\n",
        "\n",
        "                        if similarity > 0.8:  # Threshold for skill match\n",
        "                            implicit_skills.append({\n",
        "                                \"skill\": skill_name.title(),\n",
        "                                \"confidence\": similarity * 0.8,  # Lower confidence for implicit skills\n",
        "                                \"category\": category\n",
        "                            })\n",
        "\n",
        "    # Remove duplicates\n",
        "    unique_skills = {}\n",
        "    for skill in implicit_skills:\n",
        "        skill_key = skill[\"skill\"].lower()\n",
        "        if skill_key not in unique_skills or skill[\"confidence\"] > unique_skills[skill_key][\"confidence\"]:\n",
        "            unique_skills[skill_key] = skill\n",
        "\n",
        "    return list(unique_skills.values())\n",
        "\n",
        "def calculate_skill_similarity(text1, text2):\n",
        "    \"\"\"\n",
        "    Calculate similarity between two skill texts using spaCy tokenization\n",
        "    instead of NLTK's word_tokenize\n",
        "    \"\"\"\n",
        "    # Use spaCy for tokenization instead of NLTK\n",
        "    tokens1 = set(token.text.lower() for token in nlp(text1))\n",
        "    tokens2 = set(token.text.lower() for token in nlp(text2))\n",
        "\n",
        "    # Handle empty sets\n",
        "    if not tokens1 or not tokens2:\n",
        "        return 0.0\n",
        "\n",
        "    # Jaccard similarity\n",
        "    intersection = tokens1.intersection(tokens2)\n",
        "    union = tokens1.union(tokens2)\n",
        "\n",
        "    return len(intersection) / len(union)\n",
        "\n",
        "def categorize_skill(skill_name, skill_taxonomy):\n",
        "    \"\"\"\n",
        "    Categorizes a skill into one of the defined categories\n",
        "    \"\"\"\n",
        "    skill_lower = skill_name.lower()\n",
        "\n",
        "    # Check if skill exists in taxonomy\n",
        "    if skill_lower in skill_taxonomy[\"all_skills\"]:\n",
        "        return skill_taxonomy[\"all_skills\"][skill_lower]\n",
        "\n",
        "    # If not directly in taxonomy, find best category match\n",
        "    best_similarity = 0\n",
        "    best_category = \"technical\"  # Default category\n",
        "\n",
        "    for category, skills in skill_taxonomy[\"categories\"].items():\n",
        "        for category_skill in skills:\n",
        "            similarity = calculate_skill_similarity(skill_lower, category_skill.lower())\n",
        "            if similarity > best_similarity:\n",
        "                best_similarity = similarity\n",
        "                best_category = category\n",
        "\n",
        "    return best_category\n",
        "\n",
        "def infer_skills_from_experience(text, skill_taxonomy):\n",
        "    \"\"\"\n",
        "    Infers additional skills based on experience descriptions\n",
        "    \"\"\"\n",
        "    # In a real implementation, this would use a knowledge graph or ML model\n",
        "    # For this example, we'll use a simplified rule-based approach\n",
        "\n",
        "    inferred_skills = []\n",
        "\n",
        "    # Example inference rules (simplified)\n",
        "    inference_rules = [\n",
        "        {\n",
        "            \"keywords\": [\"managed\", \"team\", \"teams\", \"led\", \"supervised\", \"directed\"],\n",
        "            \"skill\": \"Leadership\",\n",
        "            \"category\": \"soft\",\n",
        "            \"confidence\": 0.7\n",
        "        },\n",
        "        {\n",
        "            \"keywords\": [\"developed\", \"created\", \"built\", \"implemented\", \"coded\"],\n",
        "            \"skill\": \"Software Development\",\n",
        "            \"category\": \"technical\",\n",
        "            \"confidence\": 0.6\n",
        "        },\n",
        "        {\n",
        "            \"keywords\": [\"analyzed\", \"data\", \"insights\", \"metrics\", \"statistics\"],\n",
        "            \"skill\": \"Data Analysis\",\n",
        "            \"category\": \"technical\",\n",
        "            \"confidence\": 0.6\n",
        "        },\n",
        "        {\n",
        "            \"keywords\": [\"design\", \"designed\", \"ui\", \"ux\", \"interface\", \"wireframes\"],\n",
        "            \"skill\": \"UI/UX Design\",\n",
        "            \"category\": \"technical\",\n",
        "            \"confidence\": 0.6\n",
        "        },\n",
        "        {\n",
        "            \"keywords\": [\"agile\", \"sprint\", \"scrum\", \"kanban\", \"jira\"],\n",
        "            \"skill\": \"Agile Methodology\",\n",
        "            \"category\": \"methodologies\",\n",
        "            \"confidence\": 0.8\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Check each inference rule\n",
        "    text_lower = text.lower()\n",
        "    for rule in inference_rules:\n",
        "        if any(keyword in text_lower for keyword in rule[\"keywords\"]):\n",
        "            # Check if this skill is already in our taxonomy results\n",
        "            if rule[\"skill\"].lower() not in [s[\"name\"].lower() for s in inferred_skills]:\n",
        "                inferred_skills.append({\n",
        "                    \"name\": rule[\"skill\"],\n",
        "                    \"source\": \"inferred\",\n",
        "                    \"confidence\": rule[\"confidence\"],\n",
        "                    \"category\": rule[\"category\"]\n",
        "                })\n",
        "\n",
        "    return inferred_skills"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcWDRnHrxH-k"
      },
      "outputs": [],
      "source": [
        "def contextualize_experience(structured_data):\n",
        "    \"\"\"\n",
        "    Analyzes and contextualizes work experience\n",
        "    \"\"\"\n",
        "    experiences = []\n",
        "\n",
        "    # Find experience section\n",
        "    experience_section = next((s for s in structured_data[\"structured_sections\"]\n",
        "                              if s[\"type\"] == \"experience\"), None)\n",
        "\n",
        "    if not experience_section:\n",
        "        return experiences\n",
        "\n",
        "    experience_text = experience_section[\"content\"]\n",
        "\n",
        "    # Split text into job entries (typically separated by blank lines or dates)\n",
        "    job_entries = re.split(r'\\n\\s*\\n', experience_text)\n",
        "    job_entries = [entry.strip() for entry in job_entries if entry.strip()]\n",
        "\n",
        "    for entry in job_entries:\n",
        "        # Extract job components using regex\n",
        "        job_title_regex = r'(?:\\b|^)(Senior|Lead|Principal|Junior|Associate)?\\s*([A-Z][a-z]+(?: [A-Z][a-z]+)*)\\b'\n",
        "        company_regex = r'\\bat\\s+([A-Z][a-z0-9]*(?:[\\s-][A-Z][a-z0-9]*)*),?\\s'\n",
        "        date_regex = r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*[\\s,-]+\\d{4}\\s*(?:(?:to|-|–|—)\\s*(?:(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*[\\s,-]+)?\\d{4}|(?:to|-|–|—)\\s*Present|Current)'\n",
        "\n",
        "        job_title_match = re.search(job_title_regex, entry)\n",
        "        company_match = re.search(company_regex, entry)\n",
        "        date_match = re.search(date_regex, entry, re.IGNORECASE)\n",
        "\n",
        "        # Extract job title\n",
        "        job_title = \"Unknown Position\"\n",
        "        if job_title_match:\n",
        "            job_title = job_title_match.group(0).strip()\n",
        "\n",
        "        # Extract company\n",
        "        company = \"Unknown Company\"\n",
        "        if company_match:\n",
        "            company = company_match.group(1).strip()\n",
        "\n",
        "        # Extract date range\n",
        "        date_range = \"Unknown Dates\"\n",
        "        if date_match:\n",
        "            date_range = date_match.group(0).strip()\n",
        "\n",
        "        # Extract description (lines that don't contain title, company or date)\n",
        "        lines = entry.split('\\n')\n",
        "        description_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            if (not job_title_match or job_title not in line) and \\\n",
        "               (not company_match or company not in line) and \\\n",
        "               (not date_match or date_match.group(0) not in line):\n",
        "                description_lines.append(line.strip())\n",
        "\n",
        "        description = '\\n'.join(description_lines)\n",
        "\n",
        "        # Create job object\n",
        "        job = {\n",
        "            \"title\": job_title,\n",
        "            \"company\": company,\n",
        "            \"dateRange\": date_range,\n",
        "            \"description\": description\n",
        "        }\n",
        "\n",
        "        # Enrich with company information (in a real implementation, this would use a database)\n",
        "        job[\"companyInfo\"] = {\n",
        "            \"industry\": infer_industry(job_title, description),\n",
        "            \"size\": \"Unknown\",\n",
        "            \"sector\": \"Unknown\"\n",
        "        }\n",
        "\n",
        "        experiences.append(job)\n",
        "\n",
        "    # Sort experiences by date (most recent first)\n",
        "    experiences.sort(key=lambda x: extract_end_year(x[\"dateRange\"]), reverse=True)\n",
        "\n",
        "    return experiences\n",
        "\n",
        "def infer_industry(title, description):\n",
        "    \"\"\"\n",
        "    Infers industry from job title and description\n",
        "    \"\"\"\n",
        "    # Map of keywords to industries\n",
        "    industry_keywords = {\n",
        "        \"Technology\": [\"software\", \"developer\", \"programming\", \"web\", \"app\", \"IT\", \"technical\", \"engineer\"],\n",
        "        \"Healthcare\": [\"medical\", \"health\", \"patient\", \"clinical\", \"doctor\", \"nurse\", \"hospital\"],\n",
        "        \"Finance\": [\"bank\", \"financial\", \"investment\", \"trading\", \"accounting\", \"finance\"],\n",
        "        \"Education\": [\"school\", \"university\", \"teaching\", \"education\", \"academic\", \"student\"],\n",
        "        \"Retail\": [\"retail\", \"store\", \"sales\", \"customer\", \"e-commerce\", \"shop\"],\n",
        "        \"Manufacturing\": [\"manufacturing\", \"production\", \"factory\", \"assembly\", \"industrial\"],\n",
        "        \"Media\": [\"media\", \"journalism\", \"publishing\", \"content\", \"news\", \"editor\"],\n",
        "        \"Consulting\": [\"consulting\", \"consultant\", \"advisory\", \"strategy\"]\n",
        "    }\n",
        "\n",
        "    # Combine title and description for analysis\n",
        "    text = (title + \" \" + description).lower()\n",
        "\n",
        "    # Count matches for each industry\n",
        "    matches = {}\n",
        "    for industry, keywords in industry_keywords.items():\n",
        "        matches[industry] = sum(1 for keyword in keywords if keyword.lower() in text)\n",
        "\n",
        "    # Return industry with most matches, or \"Unknown\" if no matches\n",
        "    max_matches = max(matches.values()) if matches else 0\n",
        "    if max_matches > 0:\n",
        "        # Get all industries with max matches\n",
        "        top_industries = [industry for industry, count in matches.items() if count == max_matches]\n",
        "        return top_industries[0]  # Return first industry with max matches\n",
        "\n",
        "    return \"Unknown\"\n",
        "\n",
        "def extract_end_year(date_range):\n",
        "    \"\"\"\n",
        "    Extracts the end year from a date range string\n",
        "    \"\"\"\n",
        "    # Extract years from the date range\n",
        "    year_matches = re.findall(r'\\d{4}', date_range)\n",
        "\n",
        "    # If \"Present\" is in the date range, use current year\n",
        "    if re.search(r'Present|Current', date_range, re.IGNORECASE):\n",
        "        return datetime.now().year\n",
        "\n",
        "    # Return the last year found, or 0 if no years found\n",
        "    return int(year_matches[-1]) if year_matches else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lh8J-A0xIA9"
      },
      "outputs": [],
      "source": [
        "def analyze_education(structured_data):\n",
        "    \"\"\"\n",
        "    Analyzes education information\n",
        "    \"\"\"\n",
        "    education = []\n",
        "    certifications = []\n",
        "\n",
        "    # Find education section\n",
        "    education_section = next((s for s in structured_data[\"structured_sections\"]\n",
        "                             if s[\"type\"] == \"education\"), None)\n",
        "\n",
        "    if education_section:\n",
        "        education_text = education_section[\"content\"]\n",
        "\n",
        "        # Split into education entries\n",
        "        entries = re.split(r'\\n\\s*\\n', education_text)\n",
        "        entries = [entry.strip() for entry in entries if entry.strip()]\n",
        "\n",
        "        for entry in entries:\n",
        "            # Extract education components using regex\n",
        "            degree_regex = r'(?:B\\.?S\\.?|B\\.?A\\.?|M\\.?S\\.?|M\\.?A\\.?|Ph\\.?D\\.?|M\\.?B\\.?A\\.?|Bachelor|Master|Doctor|Associate)[^\\n]*'\n",
        "            university_regex = r'(?:University|College|Institute|School)[^\\n]*'\n",
        "            date_regex = r'\\b\\d{4}\\b'\n",
        "            gpa_regex = r'\\bGPA\\s*(?:of|:)?\\s*(\\d+\\.\\d+|\\d+)'\n",
        "\n",
        "            degree_match = re.search(degree_regex, entry, re.IGNORECASE)\n",
        "            university_match = re.search(university_regex, entry, re.IGNORECASE)\n",
        "            date_matches = re.findall(date_regex, entry)\n",
        "            gpa_match = re.search(gpa_regex, entry, re.IGNORECASE)\n",
        "\n",
        "            # Create education entry\n",
        "            education_entry = {\n",
        "                \"degree\": degree_match.group(0).strip() if degree_match else \"Unknown Degree\",\n",
        "                \"institution\": university_match.group(0).strip() if university_match else \"Unknown Institution\",\n",
        "                \"dateRange\": '-'.join(date_matches) if date_matches else \"Unknown Dates\",\n",
        "                \"gpa\": gpa_match.group(1) if gpa_match else None\n",
        "            }\n",
        "\n",
        "            # Standardize and enrich education entry\n",
        "            standardized_entry = standardize_degree(education_entry)\n",
        "            education.append(standardized_entry)\n",
        "\n",
        "    # Find certifications section\n",
        "    cert_section = next((s for s in structured_data[\"structured_sections\"]\n",
        "                        if s[\"type\"] == \"certifications\"), None)\n",
        "\n",
        "    if cert_section:\n",
        "        cert_text = cert_section[\"content\"]\n",
        "\n",
        "        # Split into certification entries\n",
        "        cert_entries = re.split(r'\\n|•|\\*', cert_text)\n",
        "        cert_entries = [entry.strip() for entry in cert_entries if entry.strip()]\n",
        "\n",
        "        for entry in cert_entries:\n",
        "            # Extract date if present\n",
        "            date_regex = r'\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*[\\s,-]*\\d{4}\\b|\\b\\d{4}\\b'\n",
        "            date_match = re.search(date_regex, entry, re.IGNORECASE)\n",
        "\n",
        "            # Create certification entry\n",
        "            certification = {\n",
        "                \"name\": re.sub(date_regex, '', entry, flags=re.IGNORECASE).strip() if date_match else entry,\n",
        "                \"date\": date_match.group(0).strip() if date_match else \"Unknown Date\"\n",
        "            }\n",
        "\n",
        "            certifications.append(certification)\n",
        "\n",
        "    return {\n",
        "        \"education\": education,\n",
        "        \"certifications\": certifications\n",
        "    }\n",
        "\n",
        "def standardize_degree(education_entry):\n",
        "    \"\"\"\n",
        "    Standardizes degree information\n",
        "    \"\"\"\n",
        "    degree = education_entry[\"degree\"]\n",
        "\n",
        "    # Define standard degree mappings\n",
        "    degree_patterns = [\n",
        "        {\"regex\": r'\\bB\\.?S\\.?|Bachelor\\s+of\\s+Science\\b', \"standard\": \"Bachelor of Science\"},\n",
        "        {\"regex\": r'\\bB\\.?A\\.?|Bachelor\\s+of\\s+Arts\\b', \"standard\": \"Bachelor of Arts\"},\n",
        "        {\"regex\": r'\\bM\\.?S\\.?|Master\\s+of\\s+Science\\b', \"standard\": \"Master of Science\"},\n",
        "        {\"regex\": r'\\bM\\.?A\\.?|Master\\s+of\\s+Arts\\b', \"standard\": \"Master of Arts\"},\n",
        "        {\"regex\": r'\\bPh\\.?D\\.?|Doctor\\s+of\\s+Philosophy\\b', \"standard\": \"Doctor of Philosophy\"},\n",
        "        {\"regex\": r'\\bM\\.?B\\.?A\\.?|Master\\s+of\\s+Business\\s+Administration\\b', \"standard\": \"Master of Business Administration\"}\n",
        "    ]\n",
        "\n",
        "    # Find matching standard degree\n",
        "    standard_degree = degree\n",
        "    for pattern in degree_patterns:\n",
        "        if re.search(pattern[\"regex\"], degree, re.IGNORECASE):\n",
        "            standard_degree = pattern[\"standard\"]\n",
        "            break\n",
        "\n",
        "    # Extract major if present\n",
        "    major_regex = r'in\\s+([A-Za-z]+(?: [A-Za-z]+)*)\\b'\n",
        "    major_match = re.search(major_regex, degree, re.IGNORECASE)\n",
        "\n",
        "    # Determine degree level\n",
        "    level = get_degree_level(standard_degree)\n",
        "\n",
        "    return {\n",
        "        **education_entry,\n",
        "        \"degree\": standard_degree,\n",
        "        \"major\": major_match.group(1) if major_match else None,\n",
        "        \"level\": level\n",
        "    }\n",
        "\n",
        "def get_degree_level(degree):\n",
        "    \"\"\"\n",
        "    Determines the level of a degree\n",
        "    \"\"\"\n",
        "    if re.search(r'Bachelor', degree):\n",
        "        return \"undergraduate\"\n",
        "    elif re.search(r'Master|MBA', degree):\n",
        "        return \"graduate\"\n",
        "    elif re.search(r'Doctor|Ph\\.?D\\.?', degree):\n",
        "        return \"doctorate\"\n",
        "    elif re.search(r'Associate', degree):\n",
        "        return \"associate\"\n",
        "    else:\n",
        "        return \"other\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFaGxjs4xIEj"
      },
      "outputs": [],
      "source": [
        "def generate_user_profile(extracted_data):\n",
        "    \"\"\"\n",
        "    Generates a comprehensive user profile from extracted data\n",
        "    \"\"\"\n",
        "    structured_data = extracted_data[\"structured_data\"]\n",
        "    skills = extracted_data[\"skills\"]\n",
        "    experiences = extracted_data[\"experiences\"]\n",
        "    education_data = extracted_data[\"education_data\"]\n",
        "\n",
        "    # Combine all data into comprehensive user profile\n",
        "    profile = {\n",
        "        \"id\": str(uuid.uuid4()),\n",
        "        \"createdAt\": datetime.now().isoformat(),\n",
        "        \"personalInfo\": structured_data[\"contact_info\"],\n",
        "        \"skills\": {\n",
        "            \"technical\": normalize_skills(skills[\"technical\"]),\n",
        "            \"soft\": normalize_skills(skills[\"soft\"]),\n",
        "            \"languages\": normalize_skills(skills[\"languages\"]),\n",
        "            \"tools\": normalize_skills(skills[\"tools\"]),\n",
        "            \"methodologies\": normalize_skills(skills[\"methodologies\"]),\n",
        "            \"domain\": normalize_skills(skills[\"domain\"]),\n",
        "            \"inferred\": skills[\"inferred\"]\n",
        "        },\n",
        "        \"experience\": experiences,\n",
        "        \"education\": education_data[\"education\"],\n",
        "        \"certifications\": education_data[\"certifications\"],\n",
        "        \"careerStage\": determine_career_stage(experiences, education_data),\n",
        "        \"careerTrajectory\": analyze_career_trajectory(experiences)\n",
        "    }\n",
        "\n",
        "    return profile\n",
        "\n",
        "def normalize_skills(skills_array):\n",
        "    \"\"\"\n",
        "    Normalizes skills by removing duplicates and combining confidence scores\n",
        "    \"\"\"\n",
        "    skill_map = {}\n",
        "\n",
        "    # Group by skill name and calculate best confidence\n",
        "    for skill in skills_array:\n",
        "        name = skill[\"name\"].lower()\n",
        "        if name not in skill_map or skill_map[name][\"confidence\"] < skill[\"confidence\"]:\n",
        "            skill_map[name] = skill\n",
        "\n",
        "    return list(skill_map.values())\n",
        "\n",
        "def determine_career_stage(experiences, education_data):\n",
        "    \"\"\"\n",
        "    Determines career stage based on experience and education\n",
        "    \"\"\"\n",
        "    # Calculate total years of experience\n",
        "    total_years = 0\n",
        "\n",
        "    for job in experiences:\n",
        "        date_range = job[\"dateRange\"]\n",
        "\n",
        "        # Extract years\n",
        "        years = re.findall(r'\\d{4}', date_range)\n",
        "        if len(years) >= 2:\n",
        "            start_year = int(years[0])\n",
        "            end_year = datetime.now().year if \"Present\" in date_range else int(years[-1])\n",
        "            total_years += end_year - start_year\n",
        "\n",
        "    # Determine education level\n",
        "    highest_education = sorted(education_data[\"education\"],\n",
        "                              key=lambda x: {\"doctorate\": 4, \"graduate\": 3, \"undergraduate\": 2, \"associate\": 1, \"other\": 0}[x[\"level\"]],\n",
        "                              reverse=True)[0] if education_data[\"education\"] else None\n",
        "\n",
        "    # Determine career stage\n",
        "    if total_years < 2:\n",
        "        return \"entry-level\"\n",
        "    elif total_years < 5:\n",
        "        return \"early-career\"\n",
        "    elif total_years < 10:\n",
        "        return \"mid-career\"\n",
        "    elif total_years < 15:\n",
        "        return \"experienced\"\n",
        "    else:\n",
        "        return \"senior\"\n",
        "\n",
        "def analyze_career_trajectory(experiences):\n",
        "    \"\"\"\n",
        "    Analyzes career trajectory based on job history\n",
        "    \"\"\"\n",
        "    if len(experiences) < 2:\n",
        "        return {\n",
        "            \"direction\": \"stable\",\n",
        "            \"velocity\": \"normal\",\n",
        "            \"pattern\": \"linear\"\n",
        "        }\n",
        "\n",
        "    # Analyze job titles for progression\n",
        "    titles = [exp[\"title\"] for exp in experiences]\n",
        "    title_progression = analyze_title_progression(titles)\n",
        "\n",
        "    # Analyze industry changes\n",
        "    industries = [exp[\"companyInfo\"][\"industry\"] for exp in experiences if exp[\"companyInfo\"][\"industry\"] != \"Unknown\"]\n",
        "    industry_changes = len(set(industries))\n",
        "\n",
        "    # Analyze tenure patterns\n",
        "    tenures = calculate_tenures(experiences)\n",
        "\n",
        "    return {\n",
        "        \"direction\": title_progression[\"direction\"],\n",
        "        \"velocity\": determine_velocity(experiences, tenures),\n",
        "        \"pattern\": determine_pattern(industry_changes, len(experiences))\n",
        "    }\n",
        "\n",
        "def analyze_title_progression(titles):\n",
        "    \"\"\"\n",
        "    Analyzes job title progression\n",
        "    \"\"\"\n",
        "    # Define career level keywords\n",
        "    level_keywords = {\n",
        "        \"entry\": [\"assistant\", \"junior\", \"intern\", \"trainee\"],\n",
        "        \"mid\": [\"associate\", \"analyst\", \"specialist\", \"developer\"],\n",
        "        \"senior\": [\"senior\", \"lead\", \"principal\", \"manager\", \"head\", \"chief\", \"director\", \"vp\"]\n",
        "    }\n",
        "\n",
        "    # Score each title\n",
        "    scores = []\n",
        "    for title in titles:\n",
        "        title_lower = title.lower()\n",
        "        score = 0\n",
        "\n",
        "        # Check for level keywords\n",
        "        for level, keywords in level_keywords.items():\n",
        "            for keyword in keywords:\n",
        "                if keyword in title_lower:\n",
        "                    score = 1 if level == \"entry\" else 2 if level == \"mid\" else 3\n",
        "                    break\n",
        "            if score > 0:\n",
        "                break\n",
        "\n",
        "        scores.append(score or 1.5)  # Default to mid-level if no keywords found\n",
        "\n",
        "    # Calculate progression\n",
        "    first_score = scores[0]\n",
        "    last_score = scores[-1]\n",
        "\n",
        "    if last_score > first_score:\n",
        "        return {\"direction\": \"upward\", \"magnitude\": last_score - first_score}\n",
        "    elif last_score < first_score:\n",
        "        return {\"direction\": \"downward\", \"magnitude\": first_score - last_score}\n",
        "    else:\n",
        "        return {\"direction\": \"stable\", \"magnitude\": 0}\n",
        "\n",
        "def calculate_tenures(experiences):\n",
        "    \"\"\"\n",
        "    Calculates job tenures in years\n",
        "    \"\"\"\n",
        "    tenures = []\n",
        "\n",
        "    for job in experiences:\n",
        "        date_range = job[\"dateRange\"]\n",
        "\n",
        "        # Extract years\n",
        "        years = re.findall(r'\\d{4}', date_range)\n",
        "        if len(years) >= 2:\n",
        "            start_year = int(years[0])\n",
        "            end_year = datetime.now().year if \"Present\" in date_range else int(years[-1])\n",
        "            tenures.append(end_year - start_year)\n",
        "        else:\n",
        "            tenures.append(1)  # Default if unable to determine\n",
        "\n",
        "    return tenures\n",
        "\n",
        "def determine_velocity(experiences, tenures):\n",
        "    \"\"\"\n",
        "    Determines career velocity based on job changes and tenures\n",
        "    \"\"\"\n",
        "    if not tenures:\n",
        "        return \"normal\"\n",
        "\n",
        "    avg_tenure = sum(tenures) / len(tenures)\n",
        "\n",
        "    if avg_tenure < 1.5:\n",
        "        return \"rapid\"\n",
        "    elif avg_tenure < 3:\n",
        "        return \"normal\"\n",
        "    else:\n",
        "        return \"steady\"\n",
        "\n",
        "def determine_pattern(industry_changes, job_count):\n",
        "    \"\"\"\n",
        "    Determines career pattern based on industry changes\n",
        "    \"\"\"\n",
        "    if job_count == 0:\n",
        "        return \"unknown\"\n",
        "\n",
        "    change_ratio = industry_changes / job_count\n",
        "\n",
        "    if change_ratio > 0.5:\n",
        "        return \"diverse\"\n",
        "    elif change_ratio > 0.2:\n",
        "        return \"exploratory\"\n",
        "    else:\n",
        "        return \"specialized\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8JY16fUxSWX"
      },
      "outputs": [],
      "source": [
        "class CareerRecommender:\n",
        "    \"\"\"\n",
        "    Career recommendation system with deep learning capabilities\n",
        "    \"\"\"\n",
        "    def __init__(self, database_path='career_database.csv', model_path='career_model.h5'):\n",
        "        \"\"\"\n",
        "        Initialize the Career Recommender with database and ML model\n",
        "        \"\"\"\n",
        "        # Check if database exists, if not generate it\n",
        "        if not os.path.exists(database_path):\n",
        "            self.career_data = generate_career_database(database_path)\n",
        "        else:\n",
        "            self.career_data = pd.read_csv(database_path)\n",
        "\n",
        "        # Pre-process the data for career matching\n",
        "        self._preprocess_career_data()\n",
        "\n",
        "        # Load or initialize the recommendation model\n",
        "        self.model_path = model_path\n",
        "        self.model = self._load_or_create_model()\n",
        "\n",
        "        # Initialize feedback data for reinforcement learning\n",
        "        self.feedback_data = []\n",
        "\n",
        "    def _preprocess_career_data(self):\n",
        "        \"\"\"\n",
        "        Preprocess the career data for faster matching\n",
        "        \"\"\"\n",
        "        # Convert pipe-separated skills to lists\n",
        "        self.career_data['skills_list'] = self.career_data['required_skills'].apply(\n",
        "            lambda x: x.split('|')\n",
        "        )\n",
        "\n",
        "        # Create a string representation of all skills for vectorization\n",
        "        self.career_data['skills_text'] = self.career_data['required_skills'].apply(\n",
        "            lambda x: ' '.join(x.split('|'))\n",
        "        )\n",
        "\n",
        "        # Create a TF-IDF vectorizer for skill matching\n",
        "        self.vectorizer = TfidfVectorizer()\n",
        "        self.skill_vectors = self.vectorizer.fit_transform(self.career_data['skills_text'])\n",
        "\n",
        "    def _load_or_create_model(self):\n",
        "        \"\"\"\n",
        "        Load existing model or create a new one\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if os.path.exists(self.model_path):\n",
        "                print(f\"Loading existing career recommendation model from {self.model_path}\")\n",
        "                return load_model(self.model_path)\n",
        "            else:\n",
        "                print(\"Creating new career recommendation model\")\n",
        "                return self._create_model()\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {str(e)}. Creating new model.\")\n",
        "            return self._create_model()\n",
        "\n",
        "    def _create_model(self):\n",
        "        \"\"\"\n",
        "        Create a new neural network model for skill-based job matching enhancement\n",
        "        \"\"\"\n",
        "        # Simple MLP model to adjust skill match scores based on career stage and type\n",
        "        model = Sequential([\n",
        "            Dense(128, activation='relu', input_shape=(7,)),  # Input: skill match, career stage values, is_new_seeker\n",
        "            Dropout(0.3),\n",
        "            Dense(64, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "            Dense(32, activation='relu'),\n",
        "            Dense(1, activation='sigmoid')  # Output: enhanced match score\n",
        "        ])\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=0.001),\n",
        "            loss='mse',\n",
        "            metrics=['mae']\n",
        "        )\n",
        "\n",
        "        # If we had real user data, we would train the model here\n",
        "        # For now, we'll just return the untrained model\n",
        "        return model\n",
        "\n",
        "    def match_careers(self, user_profile, career_type=\"new_seeker\", top_n=10):\n",
        "        \"\"\"\n",
        "        Match the user profile with potential careers\n",
        "\n",
        "        Args:\n",
        "            user_profile: A dictionary containing the user's skills, experience, and preferences\n",
        "            career_type: 'new_seeker' or 'transitioner'\n",
        "            top_n: Number of top matches to return\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with career recommendations and learning paths\n",
        "        \"\"\"\n",
        "        # Extract user skills\n",
        "        user_skills = set()\n",
        "        for skill_category in ['technical', 'soft', 'tools', 'methodologies', 'domain', 'languages']:\n",
        "            if skill_category in user_profile['skills']:\n",
        "                user_skills.update([skill['name'].lower() for skill in user_profile['skills'][skill_category]])\n",
        "\n",
        "        # Add inferred skills\n",
        "        if 'inferred' in user_profile['skills']:\n",
        "            user_skills.update([skill['name'].lower() for skill in user_profile['skills']['inferred']])\n",
        "\n",
        "        # Create text representation of user skills for vectorization\n",
        "        user_skills_text = ' '.join(user_skills)\n",
        "\n",
        "        # Vectorize user skills\n",
        "        user_vector = self.vectorizer.transform([user_skills_text])\n",
        "\n",
        "        # Calculate similarity scores\n",
        "        similarity_scores = cosine_similarity(user_vector, self.skill_vectors).flatten()\n",
        "\n",
        "        # Add similarity scores to career data\n",
        "        career_matches = self.career_data.copy()\n",
        "        career_matches['similarity'] = similarity_scores\n",
        "\n",
        "        # Filter by career type if specified\n",
        "        if career_type in [\"new_seeker\", \"transitioner\"]:\n",
        "            career_matches = career_matches[career_matches['career_type'] == career_type]\n",
        "\n",
        "        # Filter by career stage if available\n",
        "        if 'careerStage' in user_profile:\n",
        "            # Map user career stage to our categories\n",
        "            stage_mapping = {\n",
        "                'entry-level': ['entry-level'],\n",
        "                'early-career': ['entry-level', 'early-career'],\n",
        "                'mid-career': ['early-career', 'mid-career'],\n",
        "                'experienced': ['mid-career', 'experienced'],\n",
        "                'senior': ['experienced', 'senior']\n",
        "            }\n",
        "\n",
        "            # Get appropriate career stages for the user\n",
        "            appropriate_stages = stage_mapping.get(\n",
        "                user_profile['careerStage'],\n",
        "                ['entry-level', 'early-career', 'mid-career', 'experienced', 'senior']\n",
        "            )\n",
        "\n",
        "            # Filter careers by appropriate stages\n",
        "            career_matches = career_matches[career_matches['career_stage'].isin(appropriate_stages)]\n",
        "\n",
        "        # Enhance match scores using the deep learning model\n",
        "        if len(career_matches) > 0:\n",
        "            enhanced_scores = self._enhance_match_scores(\n",
        "                career_matches,\n",
        "                user_profile,\n",
        "                career_type\n",
        "            )\n",
        "            career_matches['enhanced_similarity'] = enhanced_scores\n",
        "\n",
        "            # Sort by enhanced similarity score\n",
        "            career_matches = career_matches.sort_values('enhanced_similarity', ascending=False)\n",
        "        else:\n",
        "            # Sort by original similarity if no enhancement\n",
        "            career_matches = career_matches.sort_values('similarity', ascending=False)\n",
        "\n",
        "        # Get top N matches\n",
        "        top_matches = career_matches.head(top_n)\n",
        "\n",
        "        # Calculate skill gaps for each match\n",
        "        recommendations = []\n",
        "        all_skill_gaps = set()\n",
        "\n",
        "        for _, match in top_matches.iterrows():\n",
        "            # Get required skills for this career\n",
        "            required_skills = set(skill.lower() for skill in match['skills_list'])\n",
        "\n",
        "            # Calculate skill gaps\n",
        "            skill_gaps = required_skills - user_skills\n",
        "\n",
        "            # Calculate match percentage\n",
        "            if len(required_skills) > 0:\n",
        "                match_percentage = (len(required_skills) - len(skill_gaps)) / len(required_skills) * 100\n",
        "            else:\n",
        "                match_percentage = 100\n",
        "\n",
        "            # Adjust match percentage based on enhanced similarity if available\n",
        "            if 'enhanced_similarity' in match:\n",
        "                # Blend original match percentage with the enhanced score\n",
        "                match_percentage = 0.7 * match_percentage + 30 * match['enhanced_similarity']\n",
        "\n",
        "            # Add to recommendations\n",
        "            recommendations.append({\n",
        "                'jobTitle': match['job_title'],\n",
        "                'industry': match['industry'],\n",
        "                'matchPercentage': round(match_percentage, 1),\n",
        "                'skillsMatched': len(required_skills) - len(skill_gaps),\n",
        "                'totalSkillsRequired': len(required_skills),\n",
        "                'skillGaps': list(skill.title() for skill in skill_gaps),\n",
        "                'minSalary': int(match['min_salary']),\n",
        "                'maxSalary': int(match['max_salary']),\n",
        "                'remoteOptions': match['remote_options'],\n",
        "                'growthPotential': int(match['growth_potential']),\n",
        "                'careerType': match['career_type'],\n",
        "                'jobId': int(match['job_id'])\n",
        "            })\n",
        "\n",
        "            # Add to overall skill gaps\n",
        "            all_skill_gaps.update(skill.title() for skill in skill_gaps)\n",
        "\n",
        "        # Generate learning recommendations for skill gaps\n",
        "        learning_paths = self._generate_learning_paths(all_skill_gaps)\n",
        "\n",
        "        return {\n",
        "            'careerRecommendations': recommendations,\n",
        "            'learningPaths': learning_paths\n",
        "        }\n",
        "\n",
        "    def _enhance_match_scores(self, career_matches, user_profile, career_type):\n",
        "        \"\"\"\n",
        "        Use the deep learning model to enhance match scores based on additional factors\n",
        "        \"\"\"\n",
        "        # Extract career stage as numeric value\n",
        "        stage_values = {\n",
        "            'entry-level': 0.0,\n",
        "            'early-career': 0.25,\n",
        "            'mid-career': 0.5,\n",
        "            'experienced': 0.75,\n",
        "            'senior': 1.0\n",
        "        }\n",
        "\n",
        "        user_stage_value = stage_values.get(user_profile.get('careerStage', 'entry-level'), 0.0)\n",
        "\n",
        "        # Prepare input features for model\n",
        "        features = []\n",
        "        for _, row in career_matches.iterrows():\n",
        "            # Create feature vector for each job match\n",
        "            is_new_seeker = 1.0 if career_type == \"new_seeker\" else 0.0\n",
        "            job_stage_value = stage_values.get(row['career_stage'], 0.0)\n",
        "\n",
        "            # Extract career trajectory and experience data\n",
        "            trajectory_data = user_profile.get('careerTrajectory', {})\n",
        "            direction_value = 0.5  # neutral\n",
        "            if trajectory_data.get('direction') == 'upward':\n",
        "                direction_value = 1.0\n",
        "            elif trajectory_data.get('direction') == 'downward':\n",
        "                direction_value = 0.0\n",
        "\n",
        "            pattern_value = 0.5  # default\n",
        "            if trajectory_data.get('pattern') == 'specialized':\n",
        "                pattern_value = 1.0\n",
        "            elif trajectory_data.get('pattern') == 'diverse':\n",
        "                pattern_value = 0.0\n",
        "\n",
        "            # Create feature vector: [similarity, user_stage, job_stage, stage_diff, is_new_seeker, direction, pattern]\n",
        "            feature = [\n",
        "                row['similarity'],  # Base similarity score\n",
        "                user_stage_value,   # User career stage\n",
        "                job_stage_value,    # Job career stage\n",
        "                abs(user_stage_value - job_stage_value),  # Stage difference\n",
        "                is_new_seeker,      # If user is new job seeker\n",
        "                direction_value,    # Career trajectory direction\n",
        "                pattern_value       # Career pattern\n",
        "            ]\n",
        "\n",
        "            features.append(feature)\n",
        "\n",
        "        # If we have features, predict enhanced scores\n",
        "        if features:\n",
        "            features_array = np.array(features)\n",
        "\n",
        "            # If model is untrained, blend features in a simple way\n",
        "            # When we have real feedback data, we'd use predictions from the trained model\n",
        "            enhanced_scores = features_array[:, 0] * 0.7 + \\\n",
        "                            (1 - features_array[:, 3]) * 0.2 + \\\n",
        "                            (features_array[:, 4] if career_type == \"new_seeker\" else (1 - features_array[:, 4])) * 0.1\n",
        "\n",
        "            return enhanced_scores\n",
        "\n",
        "        # Fallback to original similarity scores\n",
        "        return career_matches['similarity'].values\n",
        "\n",
        "    def add_feedback(self, user_profile, job_id, feedback_score):\n",
        "        \"\"\"\n",
        "        Add user feedback for reinforcement learning\n",
        "\n",
        "        Args:\n",
        "            user_profile: User profile dictionary\n",
        "            job_id: ID of the job that received feedback\n",
        "            feedback_score: Score from 1-5 indicating how good the match was\n",
        "        \"\"\"\n",
        "        # Get job details\n",
        "        job = self.career_data[self.career_data['job_id'] == job_id].iloc[0] if any(self.career_data['job_id'] == job_id) else None\n",
        "\n",
        "        if job is not None:\n",
        "            # Extract features for this feedback instance\n",
        "            stage_values = {\n",
        "                'entry-level': 0.0,\n",
        "                'early-career': 0.25,\n",
        "                'mid-career': 0.5,\n",
        "                'experienced': 0.75,\n",
        "                'senior': 1.0\n",
        "            }\n",
        "\n",
        "            user_stage_value = stage_values.get(user_profile.get('careerStage', 'entry-level'), 0.0)\n",
        "            job_stage_value = stage_values.get(job['career_stage'], 0.0)\n",
        "            is_new_seeker = 1.0 if job['career_type'] == \"new_seeker\" else 0.0\n",
        "\n",
        "            # Extract trajectory data\n",
        "            trajectory_data = user_profile.get('careerTrajectory', {})\n",
        "            direction_value = 0.5\n",
        "            if trajectory_data.get('direction') == 'upward':\n",
        "                direction_value = 1.0\n",
        "            elif trajectory_data.get('direction') == 'downward':\n",
        "                direction_value = 0.0\n",
        "\n",
        "            pattern_value = 0.5\n",
        "            if trajectory_data.get('pattern') == 'specialized':\n",
        "                pattern_value = 1.0\n",
        "            elif trajectory_data.get('pattern') == 'diverse':\n",
        "                pattern_value = 0.0\n",
        "\n",
        "            # Calculate similarity score\n",
        "            user_skills = set()\n",
        "            for skill_category in ['technical', 'soft', 'tools', 'methodologies', 'domain', 'languages']:\n",
        "                if skill_category in user_profile['skills']:\n",
        "                    user_skills.update([skill['name'].lower() for skill in user_profile['skills'][skill_category]])\n",
        "\n",
        "            if 'inferred' in user_profile['skills']:\n",
        "                user_skills.update([skill['name'].lower() for skill in user_profile['skills']['inferred']])\n",
        "\n",
        "            required_skills = set(job['skills_list']) if isinstance(job['skills_list'], list) else set(job['required_skills'].split('|'))\n",
        "            skill_overlap = len(user_skills.intersection(required_skills))\n",
        "            similarity = skill_overlap / len(required_skills) if len(required_skills) > 0 else 0\n",
        "\n",
        "            # Create feature vector\n",
        "            feature = [\n",
        "                similarity,         # Base similarity score\n",
        "                user_stage_value,   # User career stage\n",
        "                job_stage_value,    # Job career stage\n",
        "                abs(user_stage_value - job_stage_value),  # Stage difference\n",
        "                is_new_seeker,      # If user is new job seeker\n",
        "                direction_value,    # Career trajectory direction\n",
        "                pattern_value       # Career pattern\n",
        "            ]\n",
        "\n",
        "            # Normalize feedback score to 0-1 range\n",
        "            normalized_score = (feedback_score - 1) / 4.0\n",
        "\n",
        "            # Add to feedback data\n",
        "            self.feedback_data.append((feature, normalized_score))\n",
        "\n",
        "            # Update model if we have enough data\n",
        "            if len(self.feedback_data) >= 10:\n",
        "                self._update_model()\n",
        "\n",
        "    def _update_model(self):\n",
        "        \"\"\"\n",
        "        Update the recommendation model with accumulated feedback\n",
        "        \"\"\"\n",
        "        if len(self.feedback_data) == 0:\n",
        "            return\n",
        "\n",
        "        # Prepare training data\n",
        "        X = np.array([f[0] for f in self.feedback_data])\n",
        "        y = np.array([f[1] for f in self.feedback_data])\n",
        "\n",
        "        # Train model with new data\n",
        "        self.model.fit(X, y, epochs=50, batch_size=8, verbose=0)\n",
        "\n",
        "        # Save updated model\n",
        "        self.model.save(self.model_path)\n",
        "        print(f\"Updated recommendation model with {len(self.feedback_data)} feedback examples\")\n",
        "\n",
        "    def _generate_learning_paths(self, skill_gaps):\n",
        "        \"\"\"\n",
        "        Generate learning recommendations for skill gaps\n",
        "        \"\"\"\n",
        "        # In a real implementation, this would query a learning resources database\n",
        "        # For this example, we'll generate sample learning resources\n",
        "\n",
        "        learning_paths = {}\n",
        "\n",
        "        for skill in skill_gaps:\n",
        "            # Generate 2-3 learning resources for each skill\n",
        "            num_resources = random.randint(2, 3)\n",
        "            resources = []\n",
        "\n",
        "            for i in range(num_resources):\n",
        "                # Determine resource type\n",
        "                resource_type = random.choice(['Course', 'Tutorial', 'Book', 'Workshop', 'Certification'])\n",
        "\n",
        "                # Determine provider based on type\n",
        "                if resource_type == 'Course':\n",
        "                    provider = random.choice(['Coursera', 'Udemy', 'edX', 'LinkedIn Learning', 'Pluralsight'])\n",
        "                elif resource_type == 'Tutorial':\n",
        "                    provider = random.choice(['YouTube', 'FreeCodeCamp', 'W3Schools', 'Khan Academy', 'TutorialsPoint'])\n",
        "                elif resource_type == 'Book':\n",
        "                    provider = random.choice(['O\\'Reilly', 'Manning', 'Packt', 'Apress', 'No Starch Press'])\n",
        "                elif resource_type == 'Workshop':\n",
        "                    provider = random.choice(['General Assembly', 'Codecademy', 'Bootcamp', 'University Extension'])\n",
        "                else:  # Certification\n",
        "                    provider = random.choice(['Microsoft', 'AWS', 'Google', 'Cisco', 'CompTIA', 'Oracle'])\n",
        "\n",
        "                # Determine difficulty\n",
        "                difficulty = random.choice(['beginner', 'intermediate', 'advanced'])\n",
        "\n",
        "                # Determine duration based on type\n",
        "                if resource_type == 'Course':\n",
        "                    duration = f\"{random.randint(4, 12)} weeks\"\n",
        "                elif resource_type == 'Tutorial':\n",
        "                    duration = f\"{random.randint(1, 10)} hours\"\n",
        "                elif resource_type == 'Book':\n",
        "                    duration = f\"{random.randint(10, 30)} hours\"\n",
        "                elif resource_type == 'Workshop':\n",
        "                    duration = f\"{random.randint(1, 5)} days\"\n",
        "                else:  # Certification\n",
        "                    duration = f\"{random.randint(1, 6)} months\"\n",
        "\n",
        "                # Create resource\n",
        "                resource = {\n",
        "                    'title': f\"{skill} {resource_type}\",\n",
        "                    'provider': provider,\n",
        "                    'type': resource_type,\n",
        "                    'duration': duration,\n",
        "                    'difficulty': difficulty,\n",
        "                    'url': f\"https://example.com/{skill.lower().replace(' ', '-')}\"\n",
        "                }\n",
        "\n",
        "                resources.append(resource)\n",
        "\n",
        "            # Sort resources by difficulty (beginner to advanced)\n",
        "            difficulty_order = {'beginner': 0, 'intermediate': 1, 'advanced': 2}\n",
        "            resources.sort(key=lambda x: difficulty_order[x['difficulty']])\n",
        "\n",
        "            learning_paths[skill] = resources\n",
        "\n",
        "        return learning_paths\n",
        "\n",
        "# After the CareerRecommender class definition and its methods\n",
        "\n",
        "def evaluate_model_ndcg(recommender, test_profiles, relevance_scores, top_k=10):\n",
        "    \"\"\"\n",
        "    Calculate Normalized Discounted Cumulative Gain (NDCG) for career recommendations\n",
        "\n",
        "    Args:\n",
        "        recommender: Your CareerRecommender instance\n",
        "        test_profiles: List of user profiles for testing\n",
        "        relevance_scores: Dictionary mapping profile_id to dict of {job_id: relevance_score}\n",
        "                         where relevance_score is typically 0-3 (0=irrelevant, 3=perfect match)\n",
        "        top_k: Number of recommendations to consider\n",
        "\n",
        "    Returns:\n",
        "        ndcg_score: Average NDCG score (0-1, higher is better)\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "\n",
        "    ndcg_scores = []\n",
        "\n",
        "    for profile in test_profiles:\n",
        "        profile_id = profile['id']\n",
        "\n",
        "        # Skip profiles without relevance judgments\n",
        "        if profile_id not in relevance_scores:\n",
        "            continue\n",
        "\n",
        "        # Get recommendations for this profile\n",
        "        recommendations = recommender.match_careers(profile, top_n=top_k)\n",
        "        recommended_jobs = [rec['jobId'] for rec in recommendations['careerRecommendations']]\n",
        "\n",
        "        # Calculate DCG (Discounted Cumulative Gain)\n",
        "        dcg = 0\n",
        "        for i, job_id in enumerate(recommended_jobs):\n",
        "            # Get relevance (0 if not in relevance scores)\n",
        "            rel = relevance_scores[profile_id].get(job_id, 0)\n",
        "            # Apply log discount (position i+1 because we're 0-indexed)\n",
        "            dcg += (2**rel - 1) / np.log2(i + 2)  # +2 because log base 2 of 1 is 0\n",
        "\n",
        "        # Calculate ideal DCG (IDCG)\n",
        "        # Sort all relevance scores for this profile in descending order\n",
        "        ideal_rels = sorted(relevance_scores[profile_id].values(), reverse=True)[:top_k]\n",
        "        idcg = sum((2**rel - 1) / np.log2(i + 2) for i, rel in enumerate(ideal_rels))\n",
        "\n",
        "        # Calculate NDCG\n",
        "        ndcg = dcg / idcg if idcg > 0 else 0\n",
        "        ndcg_scores.append(ndcg)\n",
        "\n",
        "    # Average NDCG across all profiles\n",
        "    avg_ndcg = sum(ndcg_scores) / len(ndcg_scores) if ndcg_scores else 0\n",
        "    return avg_ndcg\n",
        "\n",
        "def create_test_relevance_scores(test_profiles, all_job_ids, num_relevant=5):\n",
        "    \"\"\"\n",
        "    Create test relevance scores for NDCG evaluation\n",
        "\n",
        "    Args:\n",
        "        test_profiles: List of test user profiles\n",
        "        all_job_ids: List of all job IDs\n",
        "        num_relevant: Number of relevant jobs per profile\n",
        "\n",
        "    Returns:\n",
        "        relevance_scores: Dictionary of {profile_id: {job_id: relevance_score}}\n",
        "    \"\"\"\n",
        "    import random\n",
        "\n",
        "    relevance_scores = {}\n",
        "\n",
        "    for profile in test_profiles:\n",
        "        profile_id = profile['id']\n",
        "        relevance_scores[profile_id] = {}\n",
        "\n",
        "        # Select random jobs to be relevant\n",
        "        relevant_jobs = random.sample(all_job_ids, min(num_relevant, len(all_job_ids)))\n",
        "\n",
        "        # Assign random relevance scores (1-3) to relevant jobs\n",
        "        for job_id in relevant_jobs:\n",
        "            # Higher relevance score means more relevant (0=irrelevant, 3=perfect match)\n",
        "            relevance_scores[profile_id][job_id] = random.randint(1, 3)\n",
        "\n",
        "    return relevance_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymFiww97xSZE"
      },
      "outputs": [],
      "source": [
        "def generate_visualization_data(profile, recommendations):\n",
        "    \"\"\"\n",
        "    Generates data structures optimized for visualization\n",
        "    \"\"\"\n",
        "    # 1. Skill radar chart data\n",
        "    skill_radar_data = generate_skill_radar_data(profile['skills'])\n",
        "\n",
        "    # 2. Career path visualization\n",
        "    career_path_data = generate_career_path_data(\n",
        "        profile['experience'],\n",
        "        recommendations['careerRecommendations']\n",
        "    )\n",
        "\n",
        "    # 3. Learning pathway timeline\n",
        "    learning_pathway_data = generate_learning_pathway_data(\n",
        "        recommendations['learningPaths']\n",
        "    )\n",
        "\n",
        "    # 4. Industry compatibility chart\n",
        "    industry_compatibility_data = generate_industry_compatibility_data(\n",
        "        profile,\n",
        "        recommendations['careerRecommendations']\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'skillRadarData': skill_radar_data,\n",
        "        'careerPathData': career_path_data,\n",
        "        'learningPathwayData': learning_pathway_data,\n",
        "        'industryCompatibilityData': industry_compatibility_data\n",
        "    }\n",
        "\n",
        "def generate_visualizations(profile, recommendations):\n",
        "    \"\"\"\n",
        "    Generate all visualizations and save them to files\n",
        "\n",
        "    Args:\n",
        "        profile: User profile generated from generate_user_profile()\n",
        "        recommendations: Recommendations from CareerRecommender.match_careers()\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with paths to saved visualization files\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Get visualization data\n",
        "    visualization_data = generate_visualization_data(profile, recommendations)\n",
        "\n",
        "    # Generate each visualization\n",
        "    vis_paths = {}\n",
        "\n",
        "    # 1. Skill Radar Chart\n",
        "    skill_fig = visualize_skill_radar(visualization_data['skillRadarData'])\n",
        "    skill_path = 'skill_radar_chart.png'\n",
        "    skill_fig.savefig(skill_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close(skill_fig)\n",
        "    vis_paths['skill_radar'] = skill_path\n",
        "\n",
        "    # 2. Career Path Visualization\n",
        "    career_fig = visualize_career_path(visualization_data['careerPathData'])\n",
        "    career_path = 'career_path_visualization.png'\n",
        "    career_fig.savefig(career_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close(career_fig)\n",
        "    vis_paths['career_path'] = career_path\n",
        "\n",
        "    # 3. Learning Pathway Timeline\n",
        "    learning_fig = visualize_learning_pathway(visualization_data['learningPathwayData'])\n",
        "    learning_path = 'learning_pathway_timeline.png'\n",
        "    learning_fig.savefig(learning_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close(learning_fig)\n",
        "    vis_paths['learning_pathway'] = learning_path\n",
        "\n",
        "    # 4. Industry Compatibility Chart\n",
        "    industry_fig = visualize_industry_compatibility(visualization_data['industryCompatibilityData'])\n",
        "    industry_path = 'industry_compatibility_chart.png'\n",
        "    industry_fig.savefig(industry_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close(industry_fig)\n",
        "    vis_paths['industry_compatibility'] = industry_path\n",
        "\n",
        "    print(f\"All visualizations generated and saved:\")\n",
        "    for key, path in vis_paths.items():\n",
        "        print(f\"- {key}: {path}\")\n",
        "\n",
        "    return vis_paths\n",
        "\n",
        "def visualize_skill_radar(skills_data):\n",
        "    \"\"\"\n",
        "    Create radar chart visualization for skills proficiency\n",
        "\n",
        "    Args:\n",
        "        skills_data: Data from generate_skill_radar_data()\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "\n",
        "    # Extract categories and values\n",
        "    categories = [item['category'] for item in skills_data]\n",
        "    values = [item['value'] for item in skills_data]\n",
        "\n",
        "    # Number of variables\n",
        "    N = len(categories)\n",
        "\n",
        "    # Create angles for each category\n",
        "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
        "    angles += angles[:1]  # Close the loop\n",
        "\n",
        "    # Add values to complete the loop\n",
        "    values += values[:1]\n",
        "\n",
        "    # Set up the plot\n",
        "    fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(polar=True))\n",
        "\n",
        "    # Draw one axis per variable and add labels\n",
        "    plt.xticks(angles[:-1], categories, color='black', size=14)\n",
        "\n",
        "    # Draw the chart\n",
        "    ax.plot(angles, values, 'o-', linewidth=2, color='#4C72B0')\n",
        "    ax.fill(angles, values, color='#4C72B0', alpha=0.25)\n",
        "\n",
        "    # Configure the visualization\n",
        "    ax.set_rlabel_position(0)\n",
        "    plt.yticks([20, 40, 60, 80, 100], [\"20%\", \"40%\", \"60%\", \"80%\", \"100%\"], color=\"grey\", size=12)\n",
        "    plt.ylim(0, 100)\n",
        "\n",
        "    # Add title and styling\n",
        "    plt.title('Skill Proficiency by Category', size=20, y=1.1, fontweight='bold')\n",
        "\n",
        "    # Improve aesthetics\n",
        "    ax.spines['polar'].set_visible(False)\n",
        "    ax.grid(color='lightgray', linestyle='-', linewidth=0.5)\n",
        "\n",
        "    return fig\n",
        "\n",
        "def visualize_career_path(career_path_data):\n",
        "    \"\"\"\n",
        "    Create a career path visualization showing past experience and future possibilities\n",
        "\n",
        "    Args:\n",
        "        career_path_data: Data from generate_career_path_data()\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import networkx as nx\n",
        "\n",
        "    # Create directed graph\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # Add nodes with attributes\n",
        "    nodes = career_path_data['nodes']\n",
        "    edges = career_path_data['edges']\n",
        "\n",
        "    # Get past and future nodes\n",
        "    past_nodes = [n for n in nodes if n['type'] == 'past']\n",
        "    future_nodes = [n for n in nodes if n['type'] == 'future']\n",
        "\n",
        "    # Add nodes to graph\n",
        "    for node in nodes:\n",
        "        G.add_node(node['id'], **node)\n",
        "\n",
        "    # Add edges to graph\n",
        "    for edge in edges:\n",
        "        G.add_edge(edge['source'], edge['target'],\n",
        "                  weight=edge.get('weight', 1.0),\n",
        "                  type=edge['type'])\n",
        "\n",
        "    # Set up the plot\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # Define positions - past on left, future on right\n",
        "    pos = {}\n",
        "\n",
        "    # Position past nodes in a vertical line on the left\n",
        "    for i, node in enumerate(past_nodes):\n",
        "        pos[node['id']] = (0, len(past_nodes) - i)\n",
        "\n",
        "    # Position future nodes in a vertical line on the right\n",
        "    for i, node in enumerate(future_nodes):\n",
        "        pos[node['id']] = (3, (len(future_nodes) + 1) / (i + 1))\n",
        "\n",
        "    # Draw nodes with different colors for past and future\n",
        "    past_node_ids = [n['id'] for n in past_nodes]\n",
        "    future_node_ids = [n['id'] for n in future_nodes]\n",
        "\n",
        "    # Draw edges with different styles\n",
        "    history_edges = [(u, v) for u, v, d in G.edges(data=True) if d['type'] == 'history']\n",
        "    potential_edges = [(u, v) for u, v, d in G.edges(data=True) if d['type'] == 'potential']\n",
        "\n",
        "    # Edge styling\n",
        "    nx.draw_networkx_edges(G, pos, edgelist=history_edges, edge_color='gray',\n",
        "                          width=2, arrowsize=15, alpha=0.7)\n",
        "    nx.draw_networkx_edges(G, pos, edgelist=potential_edges, edge_color='#4C72B0',\n",
        "                          width=1.5, style='dashed', arrowsize=15, alpha=0.5)\n",
        "\n",
        "    # Node styling\n",
        "    nx.draw_networkx_nodes(G, pos, nodelist=past_node_ids, node_color='#FF7043',\n",
        "                          node_size=1000, alpha=0.9)\n",
        "    nx.draw_networkx_nodes(G, pos, nodelist=future_node_ids, node_color='#4CAF50',\n",
        "                          node_size=1000, alpha=0.7)\n",
        "\n",
        "    # Create labels with job title and company/industry\n",
        "    past_labels = {n['id']: f\"{n['title']}\\n{n['company']}\" for n in past_nodes}\n",
        "    future_labels = {n['id']: f\"{n['title']}\\n{n['industry']}\\n{n['matchPercentage']}% match\"\n",
        "                    for n in future_nodes}\n",
        "\n",
        "    # Combine labels\n",
        "    labels = {**past_labels, **future_labels}\n",
        "\n",
        "    # Draw labels\n",
        "    nx.draw_networkx_labels(G, pos, labels=labels, font_size=10, font_weight='bold')\n",
        "\n",
        "    # Add title and styling\n",
        "    plt.title('Career Path Trajectory', size=20, fontweight='bold')\n",
        "    plt.axis('off')  # Turn off axis\n",
        "\n",
        "    # Add legend\n",
        "    past_patch = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#FF7043',\n",
        "                          markersize=15, label='Past Roles')\n",
        "    future_patch = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#4CAF50',\n",
        "                            markersize=15, label='Potential Future Roles')\n",
        "    history_line = plt.Line2D([0], [0], color='gray', lw=2, label='Career History')\n",
        "    potential_line = plt.Line2D([0], [0], color='#4C72B0', lw=2, linestyle='--',\n",
        "                              label='Potential Paths')\n",
        "\n",
        "    plt.legend(handles=[past_patch, future_patch, history_line, potential_line],\n",
        "              loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=4)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def visualize_learning_pathway(learning_pathway_data):\n",
        "    \"\"\"\n",
        "    Create a timeline visualization of recommended learning pathways\n",
        "\n",
        "    Args:\n",
        "        learning_pathway_data: Data from generate_learning_pathway_data()\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.patches as patches\n",
        "\n",
        "    # Sort by start week\n",
        "    learning_items = sorted(learning_pathway_data, key=lambda x: x['startWeek'])\n",
        "\n",
        "    # Set up the plot\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # Define colors for different resource types\n",
        "    resource_colors = {\n",
        "        'Course': '#3498db',\n",
        "        'Tutorial': '#2ecc71',\n",
        "        'Book': '#9b59b6',\n",
        "        'Workshop': '#e74c3c',\n",
        "        'Certification': '#f39c12'\n",
        "    }\n",
        "\n",
        "    # Define position for each skill (y-axis)\n",
        "    skills = list(set([item['skill'] for item in learning_items]))\n",
        "    skill_positions = {skill: i for i, skill in enumerate(skills)}\n",
        "\n",
        "    # Draw timeline items\n",
        "    for item in learning_items:\n",
        "        start_week = item['startWeek']\n",
        "        end_week = item['endWeek']\n",
        "        duration = end_week - start_week\n",
        "        skill = item['skill']\n",
        "\n",
        "        # Create rectangle for each learning item\n",
        "        rect = patches.Rectangle(\n",
        "            (start_week, skill_positions[skill] - 0.3),\n",
        "            duration,\n",
        "            0.6,\n",
        "            linewidth=1,\n",
        "            edgecolor='black',\n",
        "            facecolor=resource_colors.get(item['type'], '#7f8c8d'),\n",
        "            alpha=0.7\n",
        "        )\n",
        "\n",
        "        # Add rectangle to plot\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        # Add text label in the middle of rectangle\n",
        "        if duration > 2:  # Only add text if enough space\n",
        "            ax.text(\n",
        "                start_week + duration/2,\n",
        "                skill_positions[skill],\n",
        "                item['title'],\n",
        "                ha='center',\n",
        "                va='center',\n",
        "                fontsize=9,\n",
        "                fontweight='bold',\n",
        "                color='white'\n",
        "            )\n",
        "\n",
        "    # Set y-axis labels (skills)\n",
        "    plt.yticks(list(skill_positions.values()), list(skill_positions.keys()))\n",
        "\n",
        "    # Calculate max end week for x-axis\n",
        "    max_end_week = max([item['endWeek'] for item in learning_items])\n",
        "\n",
        "    # Set x-axis labels (weeks)\n",
        "    plt.xticks(range(0, max_end_week + 2, 2), [f'Week {w}' for w in range(0, max_end_week + 2, 2)])\n",
        "\n",
        "    # Add title and styling\n",
        "    plt.title('Learning Pathway Timeline', size=20, fontweight='bold')\n",
        "    plt.xlabel('Timeline', fontsize=14)\n",
        "    plt.ylabel('Skills to Develop', fontsize=14)\n",
        "\n",
        "    # Add grid\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Add legend for resource types\n",
        "    legend_elements = [\n",
        "        patches.Patch(facecolor=color, edgecolor='black', label=res_type)\n",
        "        for res_type, color in resource_colors.items()\n",
        "    ]\n",
        "\n",
        "    plt.legend(handles=legend_elements, loc='upper center',\n",
        "              bbox_to_anchor=(0.5, -0.1), ncol=5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def visualize_industry_compatibility(compatibility_data):\n",
        "    \"\"\"\n",
        "    Create a visualization showing compatibility with different industries\n",
        "\n",
        "    Args:\n",
        "        compatibility_data: Data from generate_industry_compatibility_data()\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    import seaborn as sns\n",
        "\n",
        "    # Sort by overall compatibility\n",
        "    sorted_data = sorted(\n",
        "        compatibility_data,\n",
        "        key=lambda x: (x['skillMatch'] * 0.5 + x['personalityFit'] * 0.25 + x['valuesAlignment'] * 0.25),\n",
        "        reverse=True\n",
        "    )\n",
        "\n",
        "    # Top 5 industries for clarity\n",
        "    industries = [item['industry'] for item in sorted_data[:5]]\n",
        "    skill_matches = [item['skillMatch'] for item in sorted_data[:5]]\n",
        "    personality_fits = [item['personalityFit'] for item in sorted_data[:5]]\n",
        "    values_alignments = [item['valuesAlignment'] for item in sorted_data[:5]]\n",
        "\n",
        "    # Set up plot\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # Set width of bars\n",
        "    bar_width = 0.25\n",
        "\n",
        "    # Set positions of bars on X axis\n",
        "    r1 = np.arange(len(industries))\n",
        "    r2 = [x + bar_width for x in r1]\n",
        "    r3 = [x + bar_width for x in r2]\n",
        "\n",
        "    # Create bars\n",
        "    ax.bar(r1, skill_matches, width=bar_width, color='#3498db',\n",
        "          edgecolor='grey', label='Skill Match')\n",
        "    ax.bar(r2, personality_fits, width=bar_width, color='#2ecc71',\n",
        "          edgecolor='grey', label='Personality Fit')\n",
        "    ax.bar(r3, values_alignments, width=bar_width, color='#9b59b6',\n",
        "          edgecolor='grey', label='Values Alignment')\n",
        "\n",
        "    # Add labels and title\n",
        "    plt.xlabel('Industry', fontsize=14)\n",
        "    plt.ylabel('Compatibility Score (%)', fontsize=14)\n",
        "    plt.title('Industry Compatibility Analysis', fontsize=20, fontweight='bold')\n",
        "\n",
        "    # Add xticks on the middle of the bars\n",
        "    plt.xticks([r + bar_width for r in range(len(industries))], industries, rotation=45, ha='right')\n",
        "\n",
        "    # Create legend\n",
        "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=3)\n",
        "\n",
        "    # Add grid and improve aesthetics\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    sns.despine(left=False, bottom=False)\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def generate_skill_radar_data(skills):\n",
        "    \"\"\"\n",
        "    Generates data for skill radar chart\n",
        "    \"\"\"\n",
        "    # Define skill categories for radar chart\n",
        "    categories = [\n",
        "        'Technical', 'Tools', 'Methodologies',\n",
        "        'Domain Knowledge', 'Soft Skills', 'Languages'\n",
        "    ]\n",
        "\n",
        "    # Map categories to profile keys\n",
        "    category_map = {\n",
        "        'Technical': 'technical',\n",
        "        'Tools': 'tools',\n",
        "        'Methodologies': 'methodologies',\n",
        "        'Domain Knowledge': 'domain',\n",
        "        'Soft Skills': 'soft',\n",
        "        'Languages': 'languages'\n",
        "    }\n",
        "\n",
        "    # Calculate average confidence for each category\n",
        "    data = []\n",
        "    for category in categories:\n",
        "        profile_key = category_map[category]\n",
        "        category_skills = skills.get(profile_key, [])\n",
        "\n",
        "        if category_skills:\n",
        "            avg_confidence = sum(skill.get('confidence', 0.5) for skill in category_skills) / len(category_skills)\n",
        "        else:\n",
        "            avg_confidence = 0\n",
        "\n",
        "        data.append({\n",
        "            'category': category,\n",
        "            'value': round(avg_confidence * 100, 1)  # Convert to percentage\n",
        "        })\n",
        "\n",
        "    return data\n",
        "\n",
        "def generate_career_path_data(experience, recommendations):\n",
        "    \"\"\"\n",
        "    Generates data for career path visualization\n",
        "    \"\"\"\n",
        "    # Create nodes for past experience\n",
        "    past_nodes = []\n",
        "    for i, job in enumerate(experience):\n",
        "        past_nodes.append({\n",
        "            'id': f'past-{i}',\n",
        "            'type': 'past',\n",
        "            'title': job['title'],\n",
        "            'company': job['company'],\n",
        "            'industry': job['companyInfo']['industry'],\n",
        "            'dateRange': job['dateRange']\n",
        "        })\n",
        "\n",
        "    # Create nodes for recommended careers\n",
        "    future_nodes = []\n",
        "    for i, rec in enumerate(recommendations[:3]):  # Top 3 recommendations\n",
        "        future_nodes.append({\n",
        "            'id': f'future-{i}',\n",
        "            'type': 'future',\n",
        "            'title': rec['jobTitle'],\n",
        "            'industry': rec['industry'],\n",
        "            'matchPercentage': rec['matchPercentage'],\n",
        "            'skillGaps': rec['skillGaps']\n",
        "        })\n",
        "\n",
        "    # Create edges between nodes\n",
        "    edges = []\n",
        "\n",
        "    # Connect past experiences chronologically\n",
        "    for i in range(len(past_nodes) - 1):\n",
        "        edges.append({\n",
        "            'source': past_nodes[i]['id'],\n",
        "            'target': past_nodes[i + 1]['id'],\n",
        "            'type': 'history'\n",
        "        })\n",
        "\n",
        "    # Connect current role to future possibilities\n",
        "    if past_nodes:\n",
        "        current_role = past_nodes[-1]\n",
        "\n",
        "        for node in future_nodes:\n",
        "            edges.append({\n",
        "                'source': current_role['id'],\n",
        "                'target': node['id'],\n",
        "                'type': 'potential',\n",
        "                'weight': node['matchPercentage'] / 100\n",
        "            })\n",
        "\n",
        "    return {\n",
        "        'nodes': past_nodes + future_nodes,\n",
        "        'edges': edges\n",
        "    }\n",
        "\n",
        "def generate_learning_pathway_data(learning_paths):\n",
        "    \"\"\"\n",
        "    Generates data for learning pathway timeline\n",
        "    \"\"\"\n",
        "    timeline_items = []\n",
        "    current_week = 0\n",
        "\n",
        "    # Convert learning paths to timeline items\n",
        "    for skill, resources in learning_paths.items():\n",
        "        # Take up to 2 resources for each skill\n",
        "        for i, resource in enumerate(resources[:2]):\n",
        "            # Calculate duration in weeks (simplified)\n",
        "            duration_text = resource['duration']\n",
        "            duration_weeks = 1  # Default\n",
        "\n",
        "            if 'week' in duration_text:\n",
        "                weeks = re.search(r'(\\d+)', duration_text)\n",
        "                if weeks:\n",
        "                    duration_weeks = int(weeks.group(1))\n",
        "            elif 'month' in duration_text:\n",
        "                months = re.search(r'(\\d+)', duration_text)\n",
        "                if months:\n",
        "                    duration_weeks = int(months.group(1)) * 4\n",
        "            elif 'day' in duration_text:\n",
        "                days = re.search(r'(\\d+)', duration_text)\n",
        "                if days:\n",
        "                    duration_weeks = max(1, int(days.group(1)) // 5)\n",
        "            elif 'hour' in duration_text:\n",
        "                hours = re.search(r'(\\d+)', duration_text)\n",
        "                if hours:\n",
        "                    duration_weeks = max(1, int(hours.group(1)) // 20)\n",
        "\n",
        "            timeline_items.append({\n",
        "                'id': f'{skill}-{i}',\n",
        "                'title': resource['title'],\n",
        "                'skill': skill,\n",
        "                'provider': resource['provider'],\n",
        "                'type': resource['type'],\n",
        "                'startWeek': current_week,\n",
        "                'endWeek': current_week + duration_weeks,\n",
        "                'difficulty': resource['difficulty'],\n",
        "                'url': resource['url']\n",
        "            })\n",
        "\n",
        "            current_week += duration_weeks\n",
        "\n",
        "        # Add buffer week between skills\n",
        "        current_week += 1\n",
        "\n",
        "    return timeline_items\n",
        "\n",
        "def generate_industry_compatibility_data(profile, recommendations):\n",
        "    \"\"\"\n",
        "    Generates data for industry compatibility visualization\n",
        "    \"\"\"\n",
        "    # Extract unique industries from recommendations\n",
        "    industries = list(set(rec['industry'] for rec in recommendations))\n",
        "\n",
        "    # Calculate compatibility for each industry\n",
        "    compatibility_data = []\n",
        "\n",
        "    for industry in industries:\n",
        "        # Filter recommendations for this industry\n",
        "        industry_recs = [rec for rec in recommendations if rec['industry'] == industry]\n",
        "\n",
        "        if not industry_recs:\n",
        "            continue\n",
        "\n",
        "        # Calculate average match percentage\n",
        "        avg_match = sum(rec['matchPercentage'] for rec in industry_recs) / len(industry_recs)\n",
        "\n",
        "        # Calculate skill gap count\n",
        "        skill_gaps = set()\n",
        "        for rec in industry_recs:\n",
        "            skill_gaps.update(rec['skillGaps'])\n",
        "\n",
        "        # Check for personality fit if available\n",
        "        personality_fit = 0\n",
        "        if 'personalityMatch' in profile:\n",
        "            match = next((m for m in profile['personalityMatch'] if industry.lower() in m['career'].lower()), None)\n",
        "            if match:\n",
        "                personality_fit = match['fitScore'] * 100\n",
        "\n",
        "        # Check for values alignment if available\n",
        "        values_alignment = 0\n",
        "        if 'valuesAlignment' in profile:\n",
        "            match = next((a for a in profile['valuesAlignment'] if a['industry'] == industry), None)\n",
        "            if match:\n",
        "                values_alignment = match['alignmentScore'] * 100\n",
        "\n",
        "        compatibility_data.append({\n",
        "            'industry': industry,\n",
        "            'skillMatch': round(avg_match, 1),\n",
        "            'personalityFit': round(personality_fit, 1),\n",
        "            'valuesAlignment': round(values_alignment, 1),\n",
        "            'gapCount': len(skill_gaps)\n",
        "        })\n",
        "\n",
        "    # Sort by overall score\n",
        "    compatibility_data.sort(key=lambda x: (\n",
        "        x['skillMatch'] * 0.5 +\n",
        "        x['personalityFit'] * 0.25 +\n",
        "        x['valuesAlignment'] * 0.25\n",
        "    ), reverse=True)\n",
        "\n",
        "    return compatibility_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6f1ZApHOxSco"
      },
      "outputs": [],
      "source": [
        "def process_resume(file_path, career_type=\"new_seeker\"):\n",
        "    \"\"\"\n",
        "    Main function to process a resume PDF file\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to the resume PDF file\n",
        "        career_type: \"new_seeker\" or \"transitioner\"\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with processed resume data\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"Starting resume processing...\")\n",
        "\n",
        "        # Step 1: Validate PDF\n",
        "        validation_result = validate_pdf(file_path)\n",
        "        if not validation_result[\"valid\"]:\n",
        "            raise ValueError(f\"Invalid PDF: {validation_result['error']}\")\n",
        "\n",
        "        # Step 2: Extract text\n",
        "        print(\"Extracting text from PDF...\")\n",
        "        text_data = extract_text_from_pdf(file_path)\n",
        "\n",
        "        # Step 3: Recognize document structure\n",
        "        print(\"Recognizing document structure...\")\n",
        "        structured_data = recognize_document_structure(text_data)\n",
        "\n",
        "        # Step 4: Extract skills\n",
        "        print(\"Extracting skills...\")\n",
        "        skills = extract_skills(structured_data)\n",
        "\n",
        "        # Step 5: Contextualize experience\n",
        "        print(\"Contextualizing experience...\")\n",
        "        experiences = contextualize_experience(structured_data)\n",
        "\n",
        "        # Step 6: Analyze education\n",
        "        print(\"Analyzing education...\")\n",
        "        education_data = analyze_education(structured_data)\n",
        "\n",
        "        # Step 7: Generate profile\n",
        "        print(\"Generating user profile...\")\n",
        "        profile = generate_user_profile({\n",
        "            \"structured_data\": structured_data,\n",
        "            \"skills\": skills,\n",
        "            \"experiences\": experiences,\n",
        "            \"education_data\": education_data\n",
        "        })\n",
        "\n",
        "        # Step 8-9: Generate career recommendations using our ML-enhanced recommender\n",
        "        print(\"Generating career recommendations...\")\n",
        "        recommender = CareerRecommender()\n",
        "        recommendations = recommender.match_careers(profile, career_type=career_type)\n",
        "\n",
        "        # Step 10: Generate visualization data\n",
        "        print(\"Generating visualization data...\")\n",
        "        visualization_data = generate_visualization_data(profile, recommendations)\n",
        "\n",
        "        # New step: Generate and save visualizations\n",
        "        print(\"Creating visual representations...\")\n",
        "        visualization_paths = generate_visualizations(profile, recommendations)\n",
        "\n",
        "        # Display visualizations directly in Colab\n",
        "        from IPython.display import display, Image, Markdown\n",
        "\n",
        "        # Show career recommendations first\n",
        "        print(\"\\n--- TOP CAREER RECOMMENDATIONS ---\")\n",
        "        for i, rec in enumerate(recommendations['careerRecommendations'][:5]):\n",
        "            print(f\"{i+1}. {rec['jobTitle']} ({rec['industry']}) - {rec['matchPercentage']}% match\")\n",
        "            print(f\"   Salary range: ${rec['minSalary']:,} - ${rec['maxSalary']:,}\")\n",
        "            if rec['skillGaps']:\n",
        "                print(f\"   Skills to develop: {', '.join(rec['skillGaps'][:3])}\")\n",
        "            print()\n",
        "\n",
        "        # Display all visualizations\n",
        "        print(\"\\n--- VISUALIZATIONS ---\")\n",
        "        display(Markdown(\"### 1. Skill Radar Chart\"))\n",
        "        display(Image(visualization_paths['skill_radar']))\n",
        "\n",
        "        display(Markdown(\"### 2. Career Path Visualization\"))\n",
        "        display(Image(visualization_paths['career_path']))\n",
        "\n",
        "        display(Markdown(\"### 3. Learning Pathway Timeline\"))\n",
        "        display(Image(visualization_paths['learning_pathway']))\n",
        "\n",
        "        display(Markdown(\"### 4. Industry Compatibility Chart\"))\n",
        "        display(Image(visualization_paths['industry_compatibility']))\n",
        "\n",
        "        # Combine all results\n",
        "        result = {\n",
        "            \"profile\": profile,\n",
        "            \"recommendations\": recommendations,\n",
        "            \"visualizationData\": visualization_data,\n",
        "            \"visualizationPaths\": visualization_paths\n",
        "        }\n",
        "        print(\"Resume processing completed successfully!\")\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing resume: {str(e)}\")\n",
        "        raise e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ySo42Odb3T6s",
        "outputId": "bd14e0b7-3d1b-41cb-b1b4-f0dc8e8d6691"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Check if career database exists, if not generate it\n",
        "    if not os.path.exists('career_database.csv'):\n",
        "        print(\"Generating career database...\")\n",
        "        generate_career_database()\n",
        "\n",
        "    print(\"Welcome to Career Nexus!\")\n",
        "    print(\"Upload your resume PDF file:\")\n",
        "\n",
        "    # This is where the upload button will appear\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if uploaded:\n",
        "        # Get the filename of the uploaded file\n",
        "        resume_file = list(uploaded.keys())[0]\n",
        "\n",
        "        # Ask user about their career status\n",
        "        print(\"\\nAre you a new job seeker or transitioning to a new career?\")\n",
        "        print(\"1. New job seeker (recent graduate or entering workforce)\")\n",
        "        print(\"2. Career transitioner (changing careers or industries)\")\n",
        "\n",
        "        career_choice = input(\"Enter your choice (1 or 2): \")\n",
        "        career_type = \"new_seeker\" if career_choice == \"1\" else \"transitioner\"\n",
        "\n",
        "        print(f\"\\nProcessing resume: {resume_file} as a {career_type.replace('_', ' ')}...\")\n",
        "        result = process_resume(resume_file, career_type=career_type)\n",
        "\n",
        "        # Save results to JSON\n",
        "        output_file = 'resume_analysis_result.json'\n",
        "        with open(output_file, 'w') as f:\n",
        "            json.dump(result, f, indent=2)\n",
        "\n",
        "        print(f\"Analysis complete! Results saved to {output_file}\")\n",
        "\n",
        "        # Show key recommendations\n",
        "        print(\"\\n--- TOP CAREER RECOMMENDATIONS ---\")\n",
        "        for i, rec in enumerate(result['recommendations']['careerRecommendations'][:5]):\n",
        "            print(f\"{i+1}. {rec['jobTitle']} ({rec['industry']}) - {rec['matchPercentage']}% match\")\n",
        "            print(f\"   Salary range: ${rec['minSalary']:,} - ${rec['maxSalary']:,}\")\n",
        "            if rec['skillGaps']:\n",
        "                print(f\"   Skills to develop: {', '.join(rec['skillGaps'][:3])}\")\n",
        "            print()\n",
        "             # After displaying recommendations and visualizations, add model evaluation\n",
        "    print(\"\\n--- MODEL EVALUATION (NDCG) ---\")\n",
        "\n",
        "    # Create a recommender instance\n",
        "    eval_recommender = CareerRecommender()\n",
        "\n",
        "    # Get all job IDs from the career database\n",
        "    all_job_ids = eval_recommender.career_data['job_id'].tolist()\n",
        "\n",
        "    # Create simplified test profiles for evaluation\n",
        "    from datetime import datetime\n",
        "    test_profiles = [{\n",
        "        \"id\": f\"test_{i}\",\n",
        "        \"createdAt\": datetime.now().isoformat(),\n",
        "        \"careerStage\": \"entry-level\",\n",
        "        \"skills\": {\n",
        "            \"technical\": [{\"name\": \"Python\", \"confidence\": 0.8}],\n",
        "            \"soft\": [{\"name\": \"Communication\", \"confidence\": 0.7}],\n",
        "            \"tools\": [],\n",
        "            \"methodologies\": [],\n",
        "            \"domain\": [],\n",
        "            \"languages\": [],\n",
        "            \"inferred\": []\n",
        "        },\n",
        "        \"experience\": [],\n",
        "        \"education\": []\n",
        "    } for i in range(5)]  # Just 5 test profiles to keep evaluation fast\n",
        "\n",
        "    # Create test relevance scores\n",
        "    relevance_scores = create_test_relevance_scores(test_profiles, all_job_ids)\n",
        "\n",
        "    # Run the evaluation\n",
        "    ndcg_score = evaluate_model_ndcg(eval_recommender, test_profiles, relevance_scores)\n",
        "    print(f\"Model Quality (NDCG@10): {ndcg_score:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
